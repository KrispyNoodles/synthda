{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NVIDIA/synthda/blob/main/synthda_slowfast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn_uFXQlNIXs"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnZUtY_pM-Jz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPwjpfzCNXWx"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2fS_IoZNwxj"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install 'iopath'\n",
        "\n",
        "!pip install pytorchvideo==0.1.5 fvcore==0.1.5.post20221221\n",
        "!pip install tensorboard\n",
        "!pip install setuptools==59.5.0\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPBr84WNWW2"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-MZua-5Nrwy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorchvideo\n",
        "import torchinfo\n",
        "\n",
        "from pytorchvideo.models.hub import c2d_r50, i3d_r50, slow_r50, slowfast_r50\n",
        "from pytorchvideo.models.hub import x3d_m\n",
        "from torchvision.transforms.functional import normalize, crop, hflip\n",
        "from torchvision.transforms._functional_video import center_crop\n",
        "#from pytorchvideo.transforms.functional import short_side_scale"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pytorchvideo\n",
        "#from pytorchvideo.transforms.functional import short_side_scale"
      ],
      "metadata": {
        "id": "Y3VxgxLtrmGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def short_side_scale(video: torch.Tensor, size: int):\n",
        "    \"\"\"\n",
        "    Resize so the shorter spatial side == `size`, keeping aspect ratio.\n",
        "    video: (T, C, H, W) or (C, T, H, W) tensor\n",
        "    \"\"\"\n",
        "    from torchvision.transforms.functional import resize\n",
        "    t, c, h, w = video.shape if video.ndim == 4 else (None,)*4\n",
        "    short, long = (h, w) if h < w else (w, h)\n",
        "    new_short, new_long = size, int(size * long / short)\n",
        "    # output: (T,C,H,W)\n",
        "    return resize(video, [new_short, new_long])"
      ],
      "metadata": {
        "id": "C0vn-PJCr7FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJMNKZ1PoOSF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install iopath fvcore pytorchvideo tensorboard setuptools torchinfo opencv-python seaborn numpy Pillow scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RfaH8aLOO2c"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATr1MZFOx8r"
      },
      "outputs": [],
      "source": [
        "# function to create directories\n",
        "def create_dir(target_dir):\n",
        "    if not os.path.exists(target_dir):\n",
        "        try:\n",
        "            os.makedirs(target_dir)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_HQQUkGOQ4n"
      },
      "outputs": [],
      "source": [
        "def write_action(action_filepath,action_name,action_id,action_type,tabsize=2):\n",
        "    action_name_str = '\\tname: \"{}\"'.format(action_name)\n",
        "    action_id_str = '\\tlabel_id: {}'.format(action_id)\n",
        "    action_type_str = '\\tlabel_type: {}'.format(action_type)\n",
        "\n",
        "    with open(action_filepath, 'a') as action_file:\n",
        "        action_file.write('label {\\n')\n",
        "        action_file.write(action_name_str.expandtabs(tabsize))\n",
        "        action_file.write('\\n')\n",
        "        action_file.write(action_id_str.expandtabs(tabsize))\n",
        "        action_file.write('\\n')\n",
        "        action_file.write(action_type_str.expandtabs(tabsize))\n",
        "        action_file.write('\\n')\n",
        "        action_file.write('}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeGnbia7XZPi"
      },
      "outputs": [],
      "source": [
        "# function to get videoinfo\n",
        "def get_videoinfo(videofile):\n",
        "\n",
        "    stream = cv2.VideoCapture(videofile)\n",
        "    assert stream.isOpened(), 'Cannot capture source'\n",
        "\n",
        "    datalen = int(stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = stream.get(cv2.CAP_PROP_FPS)\n",
        "    frameSize = (int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "    fourcc = int(stream.get(cv2.CAP_PROP_FOURCC))\n",
        "    videoinfo = {'no_images': datalen, 'fps': fps, 'frameSize': frameSize, 'fourcc': decode_fourcc(fourcc)}\n",
        "\n",
        "    stream.release()\n",
        "\n",
        "    return videoinfo\n",
        "\n",
        "# function to decode fourcc\n",
        "def decode_fourcc(cc):\n",
        "    return \"\".join([chr((int(cc) >> 8 * i) & 0xFF) for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJkT5-byb9YI"
      },
      "outputs": [],
      "source": [
        "# function to split df into train and test sets\n",
        "def split(split_data,split_target,split_size,split_label_1,split_label_2):\n",
        "\n",
        "    # get the locations\n",
        "    X = split_data.drop(columns=[split_target])\n",
        "    y = split_data[split_target]\n",
        "\n",
        "    # split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=297,stratify=y)\n",
        "\n",
        "    # determine fold\n",
        "    X_train[split_target] = y_train\n",
        "    X_train['fold'] = split_label_1\n",
        "\n",
        "    X_test[split_target] = y_test\n",
        "    X_test['fold'] = split_label_2\n",
        "\n",
        "    # concat\n",
        "    split_df = pd.concat([X_train,X_test])\n",
        "\n",
        "    return split_df, X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOMx39qLeB9E"
      },
      "outputs": [],
      "source": [
        "# convert video into images\n",
        "def vid_to_img(videofile,img_dest_dir):\n",
        "\n",
        "  # create img_dest_dir\n",
        "  os.makedirs(img_dest_dir,exist_ok=True)\n",
        "\n",
        "  # read video and save images\n",
        "  img_paths = []\n",
        "  stream = cv2.VideoCapture(videofile)\n",
        "  assert stream.isOpened(), 'Cannot capture source'\n",
        "\n",
        "  datalen = int(stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  for i in tqdm(range(datalen),total=datalen):\n",
        "    (grabbed, frame) = stream.read()\n",
        "    img_path = os.path.join(img_dest_dir,'{}.jpg'.format(i))\n",
        "    cv2.imwrite(img_path,frame)\n",
        "    img_paths.append(img_path)\n",
        "\n",
        "  stream.release()\n",
        "\n",
        "  return img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoKD4hmEdD3W"
      },
      "outputs": [],
      "source": [
        "def create_frames_dir(in_df_video_info,in_frames_dir):\n",
        "\n",
        "    out_df_frame_info = pd.DataFrame(columns=['vid_id','frame_id','training_frame_path'])\n",
        "\n",
        "    # obtain video_ids\n",
        "    video_ids = list(in_df_video_info['vid_id'])\n",
        "\n",
        "    # copy images into frames_dir\n",
        "    for i, video_id in enumerate(video_ids):\n",
        "        df_video = in_df_video_info.loc[in_df_video_info['vid_id']==video_id]\n",
        "\n",
        "        if df_video.empty:\n",
        "            print(f\"Warning: No video found with vid_id {video_id}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        assert len(df_video) == 1\n",
        "\n",
        "        print('Extracting images for video {} of {}'.format(i+1,len(video_ids)))\n",
        "        img_dest_dir = os.path.join(in_frames_dir,str(video_id))\n",
        "        os.makedirs(img_dest_dir,exist_ok=True)\n",
        "\n",
        "        # obtain video_images\n",
        "        image_paths = vid_to_img(df_video['video_path'].iloc[0],img_dest_dir)\n",
        "\n",
        "        for image_path in image_paths:\n",
        "\n",
        "            # rename image\n",
        "            image_name = os.path.basename(image_path)\n",
        "            frame_id = int(image_name.split('.')[0])\n",
        "\n",
        "            # store info\n",
        "            df_dict = pd.DataFrame.from_dict({'vid_id':[video_id],\n",
        "                                              'frame_id':[frame_id],\n",
        "                                              'training_frame_path':[image_path]})\n",
        "            out_df_frame_info = pd.concat([out_df_frame_info,df_dict],ignore_index=True)\n",
        "\n",
        "    return out_df_frame_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqA2micBV29B"
      },
      "outputs": [],
      "source": [
        "def create_frame_lists_csv(in_df):\n",
        "\n",
        "    out_df = pd.DataFrame(columns=['vid_id','frame_id','rel_path'])\n",
        "\n",
        "    for i, row_i in tqdm(in_df.iterrows(),total=len(in_df)):\n",
        "\n",
        "        # obtain video info\n",
        "        vid_id = row_i['vid_id']\n",
        "        video_path = row_i['video_path']\n",
        "\n",
        "        # obtain images\n",
        "        dst_folder = os.path.join(FRAMES_DIR,str(vid_id))\n",
        "        video_images = sorted(glob.glob(str(os.path.join(dst_folder,'*.jpg'))))\n",
        "\n",
        "        for image in video_images:\n",
        "            image_name = os.path.basename(image)\n",
        "            frame_id = int(image_name.split('.')[0])\n",
        "            rel_path = str(os.path.join(str(vid_id),image_name))\n",
        "\n",
        "            # store info\n",
        "            df_dict = pd.DataFrame.from_dict({'vid_id':[vid_id],\n",
        "                                              'frame_id':[frame_id],\n",
        "                                              'rel_path':[rel_path]})\n",
        "            out_df = pd.concat([out_df,df_dict],ignore_index=True)\n",
        "\n",
        "    return out_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noeP5fZ_aFhw"
      },
      "outputs": [],
      "source": [
        "def create_annotations_csv(in_df):\n",
        "\n",
        "    out_df = pd.DataFrame(columns=['vid_id', 'frame_id',\n",
        "                                   'frame_timestamp',\n",
        "                                   'action_label'])\n",
        "\n",
        "    for i, row_i in tqdm(in_df.iterrows(),total=len(in_df)):\n",
        "\n",
        "        # obtain necessary video info\n",
        "        vid_id = row_i['vid_id']\n",
        "        action_id = row_i['action_id']\n",
        "        video_fps = row_i['fps']\n",
        "\n",
        "        # obtain images\n",
        "        dst_folder = os.path.join(FRAMES_DIR,str(vid_id))\n",
        "        video_images = sorted(glob.glob(str(os.path.join(dst_folder,'*.jpg'))))\n",
        "\n",
        "        for image in video_images:\n",
        "\n",
        "            # obtain variables\n",
        "            image_name = os.path.basename(image)\n",
        "            frame_id = int(image_name.split('.')[0].split('_')[-1])\n",
        "            frame_timestamp = frame_id / video_fps\n",
        "\n",
        "            # store\n",
        "            df_dict = pd.DataFrame.from_dict({'vid_id':[vid_id],\n",
        "                                              'frame_id':[frame_id],\n",
        "                                              'frame_timestamp':[frame_timestamp],\n",
        "                                              'action_label':[action_id]})\n",
        "\n",
        "            out_df = pd.concat([out_df,df_dict],ignore_index=True)\n",
        "\n",
        "    return out_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9z_MIAxgaLP"
      },
      "outputs": [],
      "source": [
        "def clip_sampler_random(in_last_clip_end_time,in_is_last_clip,in_frame_ids):\n",
        "    \"\"\"\n",
        "    Randomly samples clip of size CLIP_DURATION (in terms of number of frames) from video frames.\n",
        "    Args:\n",
        "        in_last_clip_end_time (int): the last frame id of last clip that was sampled\n",
        "        in_is_last_clip (int): current clip count for random sampling\n",
        "        in_frame_ids (list): list of all frame_ids for video\n",
        "    Returns:\n",
        "        out_clip_start_frame (int): starting frame id of the sampled clip\n",
        "        out_clip_end_frame (int): ending frame id of the sampled clip\n",
        "        out_is_last_clip: indicator to control when clip sampling ends\n",
        "    \"\"\"\n",
        "\n",
        "    max_possible_clip_start = max(len(in_frame_ids) - CLIP_DURATION, 0)\n",
        "\n",
        "    out_clip_start = random.randint(0, max_possible_clip_start)\n",
        "    out_clip_end = out_clip_start + CLIP_DURATION\n",
        "\n",
        "    if in_is_last_clip == None:\n",
        "        out_is_last_clip = 1\n",
        "    elif isinstance(in_is_last_clip, int) and in_is_last_clip <= 3:\n",
        "        out_is_last_clip = in_is_last_clip + 1\n",
        "    else:\n",
        "        out_is_last_clip = 'Yes'\n",
        "\n",
        "    return out_clip_start, out_clip_end, out_is_last_clip\n",
        "\n",
        "def clip_sampler_uniform(in_last_clip_end_time,in_is_last_clip,in_frame_ids):\n",
        "\n",
        "    if in_is_last_clip==None:\n",
        "        out_clip_start = 0\n",
        "        out_clip_end = CLIP_DURATION\n",
        "    else:\n",
        "        out_clip_start = in_last_clip_end_time\n",
        "        out_clip_end = out_clip_start + CLIP_DURATION\n",
        "\n",
        "    if out_clip_end + CLIP_DURATION >= len(in_frame_ids):\n",
        "        out_is_last_clip = 'Yes'\n",
        "    else:\n",
        "        out_is_last_clip = 'No'\n",
        "\n",
        "    return out_clip_start, out_clip_end, out_is_last_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU2b01rSxLsf"
      },
      "outputs": [],
      "source": [
        "def RandomShortSideScale(in_video_frames,in_min_size,in_max_size):\n",
        "    size = torch.randint(in_min_size, in_max_size + 1, (1,)).item()\n",
        "    return short_side_scale(in_video_frames, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuKY7a4DsCTe"
      },
      "outputs": [],
      "source": [
        "def read_label_map(in_label_map_file):\n",
        "    \"\"\"\n",
        "    Read label map and class ids.\n",
        "    Args:\n",
        "    in_label_map_file (str): Path to a .pbtxt containing class id's and class names\n",
        "    Returns:\n",
        "    out_label_map (dict): A dictionary mapping class id to the associated class names.\n",
        "    out_class_ids (set): A set of integer unique class id's\n",
        "    \"\"\"\n",
        "    out_label_map = {}\n",
        "    out_class_ids = set()\n",
        "    name = \"\"\n",
        "    class_id = \"\"\n",
        "    with open(in_label_map_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"  name:\"):\n",
        "                name = line.split('\"')[1]\n",
        "            elif line.startswith(\"  id:\") or line.startswith(\"  label_id:\"):\n",
        "                class_id = int(line.strip().split(\" \")[-1])\n",
        "                out_label_map[class_id] = name\n",
        "                out_class_ids.add(class_id)\n",
        "    return out_label_map, out_class_ids\n",
        "\n",
        "def load_image_lists(in_frame_paths_file, in_video_path_prefix):\n",
        "    \"\"\"\n",
        "    Loading image paths from the corresponding file.\n",
        "    Args:\n",
        "    in_frame_paths_file (str): Path to a file containing relative paths\n",
        "        to all the frames in the video. Each line in the file is of the\n",
        "        form <video_name frame_id rel_path>\n",
        "    in_video_path_prefix (str): Path to be augumented to the each relative\n",
        "        frame path to get the global frame path\n",
        "    Returns:\n",
        "    out_image_paths_list: A dictionary of list containing absolute frame paths.\n",
        "        Wherein the outer dictionary is per video and inner dictionary is per frame id.\n",
        "    \"\"\"\n",
        "\n",
        "    out_image_paths = {}\n",
        "\n",
        "    with open(in_frame_paths_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            row = line.split()\n",
        "            assert len(row) == 3\n",
        "\n",
        "            # obtain vid_id\n",
        "            vid_id = row[0]\n",
        "\n",
        "            # add info to dictionary\n",
        "            if vid_id not in out_image_paths:\n",
        "                out_image_paths[vid_id] = {}\n",
        "\n",
        "            # obtain frame_id and absolute frame paths\n",
        "            frame_id = int(row[1])\n",
        "            frame_path = os.path.join(in_video_path_prefix, row[2])\n",
        "\n",
        "            # store absolute frame paths\n",
        "            out_image_paths[vid_id][frame_id] = frame_path\n",
        "\n",
        "    # sort frame_paths by frame_id\n",
        "    out_image_paths_list = {}\n",
        "    for vid_id in out_image_paths:\n",
        "        out_image_paths_list[vid_id] = {}\n",
        "        sorted_frame_ids = sorted(out_image_paths[vid_id])\n",
        "        for frame_id in sorted_frame_ids:\n",
        "            out_image_paths_list[vid_id][frame_id] = out_image_paths[vid_id][frame_id]\n",
        "\n",
        "    return out_image_paths_list\n",
        "\n",
        "def load_and_parse_labels_csv(in_frame_labels_file,in_allowed_class_ids=None):\n",
        "    \"\"\"\n",
        "    Parses Kinetics per frame labels .csv file.\n",
        "    Args:\n",
        "    in_frame_labels_file (str): Path to the file containing labels per key frame. The file format is given by\n",
        "        <video_name, frame_id, frame_timestamp, action_label>\n",
        "    in_allowed_class_ids (set): A set of integer unique class id's that are allowed in the dataset.\n",
        "        If none, all class id's are allowed in the bbox labels.\n",
        "    Returns:\n",
        "    out_labels_dict: A dictionary of dictionary containing labels per each keyframe in each video.\n",
        "        Here, the label for each keyframe is again a dict of the form,\n",
        "        {\n",
        "            'frame_timestamp': timestamp of keyframe\n",
        "            'labels': a list of action labels for the bounding box\n",
        "        }\n",
        "    \"\"\"\n",
        "    out_labels_dict = {}\n",
        "    with open(in_frame_labels_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            row = line.strip().split(\",\")\n",
        "            assert len(row) == 4\n",
        "\n",
        "            # obtain info\n",
        "            vid_id = row[0]\n",
        "            frame_id = int(row[1])\n",
        "            frame_timestamp = float(row[2])\n",
        "            label = -1 if row[3] == \"\" else int(row[3])\n",
        "\n",
        "            # Continue if the current label is not in allowed labels\n",
        "            if (in_allowed_class_ids is not None) and (label not in in_allowed_class_ids):\n",
        "                continue\n",
        "\n",
        "            # add info to dictionaries\n",
        "            if vid_id not in out_labels_dict:\n",
        "                out_labels_dict[vid_id] = {}\n",
        "            if frame_id not in out_labels_dict[vid_id]:\n",
        "                out_labels_dict[vid_id][frame_id] = {}\n",
        "\n",
        "            out_labels_dict[vid_id][frame_id][\"frame_timestamp\"] = frame_timestamp\n",
        "            out_labels_dict[vid_id][frame_id][\"labels\"] = label\n",
        "\n",
        "    return out_labels_dict\n",
        "\n",
        "def read_kinetics_data_from_csv(in_frame_paths_file,in_frame_labels_file,in_video_path_prefix,in_label_map_file=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_frame_paths_file (str): Path to a file containing relative paths\n",
        "            to all the frames in the video. Each line in the file is of the form\n",
        "                <video_name frame_id rel_path>\n",
        "        in_frame_labels_file (str): Path to the file containing containing labels\n",
        "            per key frame. The file format is given by\n",
        "                <video_name, frame_id, frame_timestamp, bbox_x_1, bbox_y_1, bbox_x_2, bbox_y_2, action_label> #bbox info not used\n",
        "        in_video_path_prefix (str): Path to be augumented to the each relative frame\n",
        "            path to get the global frame path.\n",
        "        in_label_map_file (str): Path to a .pbtxt containing class id's and class names.\n",
        "            If not defined, label_map is not loaded and bbox labels are not pruned based on allowable class_id's in label_map.\n",
        "    Returns:\n",
        "        out_labeled_frame_paths: A dictionary of dictionary containing labels per each keyframe in each video.\n",
        "            Here, the label for each keyframe is again a dict of the form,\n",
        "            {\n",
        "                'frame_path': absolute location of video frame\n",
        "                'frame_timestamp': timestamp of the keyframe\n",
        "                'labels': a list of action labels for the bounding box\n",
        "            }\n",
        "    \"\"\"\n",
        "    if in_label_map_file is not None:\n",
        "        _, allowed_class_ids = read_label_map(in_label_map_file)\n",
        "    else:\n",
        "        allowed_class_ids = None\n",
        "\n",
        "    # load image paths\n",
        "    image_paths = load_image_lists(in_frame_paths_file, in_video_path_prefix)\n",
        "\n",
        "    # load frame labels\n",
        "    frame_labels = load_and_parse_labels_csv(in_frame_labels_file,in_allowed_class_ids=allowed_class_ids)\n",
        "\n",
        "    # combine all info for output\n",
        "    out_labeled_frame_paths = {}\n",
        "    for vid_id in image_paths:\n",
        "        out_labeled_frame_paths[vid_id] = {}\n",
        "        for frame_id in image_paths[vid_id]:\n",
        "\n",
        "            # get frame timestamp, labels\n",
        "            labels_info_dict = frame_labels[vid_id][frame_id]\n",
        "\n",
        "            # add frame path\n",
        "            labels_info_dict[\"frame_path\"] = image_paths[vid_id][frame_id]\n",
        "\n",
        "            # store information\n",
        "            out_labeled_frame_paths[vid_id][frame_id] = labels_info_dict\n",
        "\n",
        "    return out_labeled_frame_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqwk-EIf2Le8"
      },
      "outputs": [],
      "source": [
        "def thwc_to_cthw(in_tensor):\n",
        "    \"\"\"\n",
        "    Permute tensor from (time, height, width, channel) to (channel, time, height, width).\n",
        "    \"\"\"\n",
        "    return in_tensor.permute(3, 0, 1, 2)\n",
        "\n",
        "def load_clip_frames(in_clip_start,in_clip_end,in_frames_info_dict):\n",
        "    '''\n",
        "    Args:\n",
        "        in_clip_start_frame (int): starting frame id of clip\n",
        "        in_clip_end_frame (int): ending frame id of clip\n",
        "        in_frames_info_dict (dictionary): Here, the frame_ids serve as keys to a dict of the form,\n",
        "            {\n",
        "                'frame_path': absolute location of video frame\n",
        "                'frame_timestamp': timestamp of the keyframe\n",
        "                'labels': a list of action labels for the bounding box\n",
        "            }\n",
        "    Returns:\n",
        "        out_clip_dict (dictionary):\n",
        "            {\n",
        "                'video': a list of video frames\n",
        "                'labels': a list of action labels for each bounding box\n",
        "            }\n",
        "\n",
        "    '''\n",
        "    out_video_frames = []\n",
        "    out_labels = []\n",
        "\n",
        "    for frame_id in range(in_clip_start,in_clip_end):\n",
        "\n",
        "        # get info\n",
        "        frame_path = in_frames_info_dict[frame_id]['frame_path']\n",
        "        frame_label = in_frames_info_dict[frame_id]['labels']\n",
        "\n",
        "        # read image\n",
        "        img_bgr = cv2.imread(frame_path)\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # store\n",
        "        out_video_frames.append(img_rgb)\n",
        "        out_labels.append([frame_label])\n",
        "\n",
        "    # check\n",
        "    assert len(out_video_frames) == CLIP_DURATION\n",
        "\n",
        "    # convert to tensor with right shape\n",
        "    out_video_frames = torch.as_tensor(np.stack(out_video_frames))\n",
        "    out_video_frames = thwc_to_cthw(out_video_frames)\n",
        "\n",
        "    out_clip_dict = {\"video\": out_video_frames,\n",
        "                    \"labels\": out_labels}\n",
        "\n",
        "    return out_clip_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYk2uSNJsp-N"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "\n",
        "class MultiProcessSampler(DistributedSampler):\n",
        "    \"\"\"\n",
        "    MultiProcessSampler handles the storage, loading, decoding and clip sampling for a video dataset.\n",
        "    It assumes each video is stored as a frame video (e.g. a folder of jpg, or png)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          dataset: An iterable dataset\n",
        "        \"\"\"\n",
        "        super().__init__(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adov88uZzO9Z"
      },
      "outputs": [],
      "source": [
        "class KineticsVideoDataset(torch.utils.data.IterableDataset):\n",
        "    \"\"\"\n",
        "    KineticsVideoDataset handles the storage, loading, decoding and clip sampling for a video dataset.\n",
        "    It assumes each video is stored as a frame video (e.g. a folder of jpg, or png)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,in_labeled_frame_paths,in_clip_sampler,in_video_sampler,in_transform_fn=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          in_labeled_frame_paths: A dictionary of dictionary containing labels per each keyframe in each video.\n",
        "            Here, the label for each keyframe is again a dict of the form,\n",
        "            {\n",
        "                'frame_path': absolute location of video frame\n",
        "                'frame_timestamp': timestamp of the keyframe\n",
        "                'labels': a list of action labels for the bounding box\n",
        "            }\n",
        "          in_clip_sampler (ClipSampler): Defines how clips should be sampled from each video.\n",
        "          in_video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal video container.\n",
        "            This defines the order videos are decoded and, if necessary, the distributed split.\n",
        "          in_transform_fn (Callable): This callable is evaluated on the dataset output before\n",
        "            the dataset is returned. It can be used for user defined preprocessing and\n",
        "            augmentations on the clips. The dataset output format is described in __next__()\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize inputs\n",
        "        self._MAX_CONSECUTIVE_FAILURES = 10\n",
        "        self._labeled_videos = in_labeled_frame_paths\n",
        "        self._clip_sampler = in_clip_sampler\n",
        "\n",
        "        # Deal with video sampler\n",
        "        # If a RandomSampler is used we need to pass in a custom random generator that ensures all PyTorch multiprocess workers have the same random seed.\n",
        "        self._video_sampler_random_generator = None\n",
        "        if in_video_sampler == torch.utils.data.RandomSampler:\n",
        "            self._video_sampler_random_generator = torch.Generator()\n",
        "            self._video_sampler = in_video_sampler(self._labeled_videos, generator=self._video_sampler_random_generator)\n",
        "        else:\n",
        "            self._video_sampler = in_video_sampler(self._labeled_videos)\n",
        "\n",
        "        self._transform = in_transform_fn\n",
        "\n",
        "        # Initialize other variables needed\n",
        "        self._video_sampler_iter = None  # Initialized on first call at self.__next__()\n",
        "\n",
        "        # Depending on the clip sampler type, we may want to sample multiple clips\n",
        "        # from one video. In that case, we keep the stored video, label and previous sampled\n",
        "        # clip time in these variables.\n",
        "        self._loaded_video = None\n",
        "        self._loaded_clip = None\n",
        "        self._last_clip_end_time = None\n",
        "        self._is_last_clip = None\n",
        "\n",
        "    @property\n",
        "    def num_videos(self):\n",
        "        \"\"\"\n",
        "        Returns: Number of videos in dataset\n",
        "        \"\"\"\n",
        "        return len(self._video_sampler)\n",
        "\n",
        "    @property\n",
        "    def num_clips(self):\n",
        "        \"\"\"\n",
        "        Returns: Number of clips in dataset\n",
        "        \"\"\"\n",
        "\n",
        "        if self._clip_sampler == clip_sampler_random:\n",
        "            return len(self._video_sampler) * 5\n",
        "        elif self._clip_sampler == clip_sampler_uniform:\n",
        "            total_clips_count = 0\n",
        "            for key, value in self._labeled_videos.items():\n",
        "                no_of_clips = len(value) // CLIP_DURATION\n",
        "                total_clips_count+= no_of_clips\n",
        "            return total_clips_count\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._video_sampler_iter = None  # Reset video sampler\n",
        "\n",
        "        # If we're in a PyTorch DataLoader multiprocessing context, we need to use the same seed for each worker's RandomSampler generator.\n",
        "        # The workers at each __iter__ call are created from the unique value: worker_info.seed - worker_info.id, which we can use for this seed.\n",
        "        worker_info = torch.utils.data.get_worker_info() #  If worker_info is None, then this is single-process data loading\n",
        "        if self._video_sampler_random_generator is not None and worker_info is not None:\n",
        "            base_seed = worker_info.seed - worker_info.id\n",
        "            self._video_sampler_random_generator.manual_seed(base_seed)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Retrieves the next dataset based on the video sampler and clip sampling strategy.\n",
        "        Returns: A dictionary with the following format.\n",
        "        sample_dict = {\"vid_id\": id of video,\n",
        "                      \"clip_index\": clip_index (frame_id of first frame in clip),\n",
        "                      \"video\": video frames,\n",
        "                      \"labels\": labels\n",
        "                        }\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned\n",
        "        if not self._video_sampler_iter:\n",
        "            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))\n",
        "\n",
        "        # try to load next dataset for _MAX_CONSECUTIVE_FAILURES\n",
        "        for i_try in range(self._MAX_CONSECUTIVE_FAILURES):\n",
        "\n",
        "            # Reuse previously stored video if there are still clips to be sampled from the last loaded video\n",
        "            if self._loaded_video:\n",
        "                vid_id, frame_ids, frames_info_dict = self._loaded_video\n",
        "            else:\n",
        "                video_idx = next(self._video_sampler_iter)\n",
        "                try:\n",
        "                    # get info for frames (dictionary of dictionary)\n",
        "                    # Key - frame_id (int):\n",
        "                    # Value - {'frame_timestamp': float,\n",
        "                    #'labels': action_id (int),\n",
        "                    #'frame_path': absolute frame path (str)}\n",
        "\n",
        "                    vid_id = list(self._labeled_videos.keys())[video_idx]\n",
        "                    frames_info_dict = self._labeled_videos[vid_id]\n",
        "                    frame_ids = sorted(frames_info_dict)\n",
        "\n",
        "                    self._loaded_video = (vid_id, frame_ids, frames_info_dict)\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # subsample video for clips\n",
        "            clip_start,clip_end,is_last_clip = self._clip_sampler(self._last_clip_end_time,self._is_last_clip,frame_ids)\n",
        "\n",
        "            # load the next clip\n",
        "            self._loaded_clip = load_clip_frames(clip_start,clip_end,frames_info_dict)\n",
        "            self._last_clip_end_time = clip_end\n",
        "            self._is_last_clip = is_last_clip\n",
        "\n",
        "            # store necessary outputs\n",
        "            sample_dict = {\"vid_id\": vid_id,\n",
        "                           \"clip_index\": clip_start,\n",
        "                           \"video\": self._loaded_clip['video'],\n",
        "                           \"labels\": self._loaded_clip['labels']}\n",
        "\n",
        "            # carry out transformation\n",
        "            if self._transform is not None:\n",
        "                sample_dict = self._transform(sample_dict)\n",
        "\n",
        "            # Close the loaded video if last clip and reset parameters\n",
        "            if is_last_clip=='Yes':\n",
        "                self._loaded_video = None\n",
        "                self._loaded_clip = None\n",
        "                self._last_clip_end_time = None\n",
        "                self._is_last_clip = None\n",
        "\n",
        "                # Force garbage collection to release video container immediately otherwise memory can spike.\n",
        "                gc.collect()\n",
        "\n",
        "            # return sample_dict as next dataset\n",
        "            return sample_dict\n",
        "\n",
        "        # raise error after running through i_tries\n",
        "        else:\n",
        "            raise RuntimeError(f\"Failed to load video after {self._MAX_CONSECUTIVE_FAILURES} retries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4JnHT9jjpI-"
      },
      "outputs": [],
      "source": [
        "# helper function to create Kinetics dataset\n",
        "def create_kinetics_dataset(in_frame_paths_file,in_frame_labels_file,in_video_path_prefix,in_clip_sampler,in_video_sampler,\n",
        "                       in_label_map_file=None,in_transform_fn=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_frame_paths_file (str): Path to a file containing relative paths\n",
        "            to all the frames in the video. Each line in the file is of the form\n",
        "                <video_name frame_id rel_path>\n",
        "        in_frame_labels_file (str): Path to the file containing containing labels\n",
        "            per key frame. The file format is given by\n",
        "                <video_name, frame_id, frame_timestamp, bbox_x_1, bbox_y_1, bbox_x_2, bbox_y_2, action_label> #bbox info not used\n",
        "        in_video_path_prefix (str): Path to be augumented to the each relative frame\n",
        "            path to get the global frame path.\n",
        "        in_clip_sampler (ClipSampler): Defines how clips should be sampled from each video.\n",
        "        in_video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal video container.\n",
        "            This defines the order videos are decoded and, if necessary, the distributed split.\n",
        "        in_label_map_file (str): Path to a .pbtxt containing class id's and class names.\n",
        "            If not defined, label_map is not loaded and bbox labels are not pruned based on allowable class_id's in label_map.\n",
        "        in_transform_fn (Optional[Callable]): This callable is evaluated on the clip output before the clip are returned.\n",
        "            It can be used for user defined preprocessing and augmentations to the clips.\n",
        "            If transform is None, the clips are returned as it is.\n",
        "    \"\"\"\n",
        "\n",
        "    labeled_frame_paths = read_kinetics_data_from_csv(in_frame_paths_file,in_frame_labels_file,in_video_path_prefix,in_label_map_file)\n",
        "\n",
        "    return KineticsVideoDataset(in_labeled_frame_paths=labeled_frame_paths,\n",
        "                           in_clip_sampler=in_clip_sampler,in_video_sampler=in_video_sampler,\n",
        "                           in_transform_fn=in_transform_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed8KqdvM_ukE"
      },
      "outputs": [],
      "source": [
        "def temporal_subsample(in_video_frames,in_labels,num_samples,temporal_dim=-3):\n",
        "    '''\n",
        "    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n",
        "    When num_samples is larger than the size of temporal dimension of the video,\n",
        "    it will sample frames based on nearest neighbor interpolation\n",
        "    Args:\n",
        "        in_video_frames (torch tensor): A video tensor with a temporal dimension\n",
        "        in_labels (list): list of labels correspoinding to each video frame\n",
        "        num_samples (int): The number of equispaced samples to be selected\n",
        "        temporal_dim (int): dimension of temporal to perform temporal subsample\n",
        "    Returns:\n",
        "        Corresponding subsampled temporal outputs\n",
        "    '''\n",
        "\n",
        "    t = in_video_frames.shape[temporal_dim]\n",
        "    assert num_samples > 0 and t > 0\n",
        "\n",
        "    # Sample by nearest neighbor interpolation if num_samples > t\n",
        "    indices = torch.linspace(0,t-1,num_samples)\n",
        "    indices =  torch.clamp(indices,0,t-1).long()\n",
        "\n",
        "    # Carry out sampling\n",
        "    out_video_frames = torch.index_select(in_video_frames,temporal_dim,indices)\n",
        "    out_labels = in_labels[indices]\n",
        "\n",
        "    return out_video_frames,out_labels\n",
        "\n",
        "def Normalize(in_video_frames,in_mean,in_std):\n",
        "    out_video_frames = in_video_frames.permute(1, 0, 2, 3)  # CTHW to TCHW\n",
        "    out_video_frames = normalize(out_video_frames, mean=in_mean, std=in_std)\n",
        "    out_video_frames = out_video_frames.permute(1, 0, 2, 3)  # TCHW to CTHW\n",
        "\n",
        "    return out_video_frames\n",
        "\n",
        "\n",
        "def ShortSideScale(\n",
        "    x: torch.Tensor,\n",
        "    in_size: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Determines the shorter spatial dim of the video (i.e. width or height) and scales\n",
        "    it to the given size. To maintain aspect ratio, the longer side is then scaled\n",
        "    accordingly.\n",
        "    Args:\n",
        "        x (torch.Tensor): A video tensor of shape (C, T, H, W) and type torch.float32.\n",
        "        size (int): The size the shorter side is scaled to.\n",
        "        interpolation (str): Algorithm used for upsampling,\n",
        "            options: nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'\n",
        "    Returns:\n",
        "        An x-like Tensor with scaled spatial dims.\n",
        "    \"\"\"\n",
        "    assert len(x.shape) == 4\n",
        "    assert x.dtype == torch.float32\n",
        "    c, t, h, w = x.shape\n",
        "\n",
        "    if w < h:\n",
        "        new_h = int(math.floor((float(h) / w) * size))\n",
        "        new_w = size\n",
        "    else:\n",
        "        new_h = size\n",
        "        new_w = int(math.floor((float(w) / h) * size))\n",
        "\n",
        "    return torch.nn.functional.interpolate(x, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tra3s2ri8BBp"
      },
      "outputs": [],
      "source": [
        "# for data transformations\n",
        "def transform_fn(single_input):\n",
        "\n",
        "    \"\"\"\n",
        "      works on a record level\n",
        "    \"\"\"\n",
        "\n",
        "    video_index, clip_index, video, labels = single_input[\"vid_id\"], single_input[\"clip_index\"], single_input[\"video\"], single_input[\"labels\"]\n",
        "\n",
        "    # convert labels to arrays\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Sample frames together with labels\n",
        "    video, labels = temporal_subsample(video,labels,NUM_FRAMES,temporal_dim=-3)\n",
        "\n",
        "    # Normalize the video; [0, 255] --> [0, 1]\n",
        "    video = video.float()\n",
        "    video = video / 255.0\n",
        "    height, width = video.shape[2], video.shape[3]\n",
        "\n",
        "    # Normalize images by mean and std\n",
        "    video = Normalize(video, in_mean=np.array(MEAN, dtype=np.float32), in_std=np.array(STD, dtype=np.float32))\n",
        "\n",
        "\n",
        "    if MODEL == \"x3d_m\":\n",
        "\n",
        "      # Short Side Scale\n",
        "      video = ShortSideScale(video,in_size=SIDE_SIZE)\n",
        "\n",
        "      # Center Crop Video\n",
        "      video = center_crop(video, (CROP_SIZE,CROP_SIZE))\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "       # Random Short Side Scale\n",
        "       video = RandomShortSideScale(video,in_min_size=MIN_SIDE_SIZE,in_max_size=MAX_SIDE_SIZE)\n",
        "\n",
        "       # Random Crop\n",
        "       video = RandomCrop(video,in_crop_size=CROP_SIZE)\n",
        "\n",
        "       # Random Horizontal Flip\n",
        "       video = RandomHorizontalFlip(video,in_p=0.5)\n",
        "\n",
        "    # Incase of slowfast, generate both pathways\n",
        "    if MODEL == \"slowfast\":\n",
        "\n",
        "         fast_pathway = video\n",
        "\n",
        "         # Perform temporal sampling from the fast pathway.\n",
        "         slow_pathway = torch.index_select(video,1,torch.linspace(0, video.shape[1] - 1, video.shape[1] // SLOWFAST_ALPHA).long())\n",
        "\n",
        "         video = [slow_pathway, fast_pathway]\n",
        "\n",
        "    return video_index, clip_index, video, torch.from_numpy(np.array(labels)).type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvv5HkgCiaCt"
      },
      "outputs": [],
      "source": [
        "# collate function\n",
        "def collate_fn(batch):\n",
        "\n",
        "    if MODEL in [\"c2d\",\"i3d\",\"slow\"]:\n",
        "\n",
        "        video_names_merged, clip_indexes_merged, videos_merged, labels_merged = [], [], [], []\n",
        "\n",
        "        for clip_i, clip in enumerate(batch):\n",
        "\n",
        "            video, labels = clip\n",
        "            video_names_merged.append(clip_i)  # Using clip index as a placeholder for video name\n",
        "            videos_merged.append(video)  # Video tensor\n",
        "            labels_merged.append(labels)  # Label tensor\n",
        "            clip_indexes_merged.append(clip_i)  # Clip index\n",
        "\n",
        "        videos_merged = torch.stack(videos_merged)\n",
        "        labels_merged = torch.vstack(labels_merged)\n",
        "\n",
        "    elif MODEL == \"slowfast\":\n",
        "\n",
        "        video_names_merged, clip_indexes_merged, slow_merged, fast_merged, labels_merged = [], [], [], [], []\n",
        "\n",
        "        for clip_i, clip in enumerate(batch):\n",
        "\n",
        "            name, clip_index, video, labels = clip\n",
        "\n",
        "            video_names_merged.append(name)\n",
        "            slow_merged.append(video[0])\n",
        "            fast_merged.append(video[1])\n",
        "            labels_merged.append(labels)\n",
        "            clip_indexes_merged.append(clip_index)\n",
        "\n",
        "        slow_merged = torch.stack(slow_merged)\n",
        "        fast_merged = torch.stack(fast_merged)\n",
        "        videos_merged = [slow_merged, fast_merged]\n",
        "        labels_merged = torch.vstack(labels_merged)\n",
        "\n",
        "    return video_names_merged, clip_indexes_merged, videos_merged, labels_merged"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# added collate function for tuples for synthda\n",
        "\"\"\"\n",
        "def dict_collate_fn(batch):\n",
        "\n",
        "    Converts a list of tuples from Dataset.__getitem__()\n",
        "    into a single dictionary batch for the training loop.\n",
        "    Assumes each item in `batch` is either:\n",
        "        (video_tensor, label_int)  OR\n",
        "        (video_tensor, label_int, metadata...)\n",
        "\n",
        "    videos , labels_full = zip(*[(b[0], b[3]) for b in batch]) # ignore any extras\n",
        "\n",
        "    videos  = torch.stack(videos).float()          # [B, C, T, H, W]\n",
        "    labels  = torch.stack([l[0] for l in labels_full]).long()  # [B]\n",
        "\n",
        "    return {\"video\": videos, \"label\": labels}\n",
        "\"\"\"\n",
        "\n",
        "def dict_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Convert PyTorchVideo tuples into the dict expected by the training loop.\n",
        "    Handles single-path and SlowFast two-path inputs.\n",
        "    Returns\n",
        "    -------\n",
        "    dict(video=Tensor or list[Tensor], label=Tensor)\n",
        "    \"\"\"\n",
        "    videos, labels = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        # item is either (video, label) or (name, clip_idx, video, label)\n",
        "        video = item[0] if len(item) == 2 else item[2]\n",
        "        label = item[1] if len(item) == 2 else item[3]\n",
        "\n",
        "        # ── CASE A: video is already a tensor ───────────────────────────\n",
        "        if torch.is_tensor(video):\n",
        "            videos.append(video)\n",
        "        # ── CASE B: SlowFast – list of two tensors ─────────────────────\n",
        "        elif isinstance(video, list) and torch.is_tensor(video[0]):\n",
        "            # stack slow tensors together, fast tensors together\n",
        "            if not videos:\n",
        "                videos = [[], []]          # videos[0] = slow, videos[1] = fast\n",
        "            videos[0].append(video[0])\n",
        "            videos[1].append(video[1])\n",
        "        # ── CASE C: plain Python list/ndarray ───────────────────────────\n",
        "        else:\n",
        "            videos.append(torch.tensor(video))\n",
        "\n",
        "        # collapse per-frame labels -> single class id\n",
        "        if torch.is_tensor(label):\n",
        "            if label.ndim > 1:\n",
        "                label = torch.mode(label.squeeze(), 0).values\n",
        "            label = label.long()\n",
        "        else:\n",
        "            label = torch.tensor(label).long()\n",
        "        labels.append(label)\n",
        "\n",
        "    # -------- convert videos to batched tensor(s) ----------------------\n",
        "    if isinstance(videos, list) and len(videos) == 2 and isinstance(videos[0], list):\n",
        "        # SlowFast: stack slow path and fast path separately\n",
        "        slow_batch = torch.stack(videos[0]).float()\n",
        "        fast_batch = torch.stack(videos[1]).float()\n",
        "        videos = [slow_batch, fast_batch]\n",
        "    else:\n",
        "        videos = torch.stack(videos).float()   # single-path\n",
        "\n",
        "    labels = torch.stack(labels).long()        # [B]\n",
        "\n",
        "    return {\"video\": videos, \"label\": labels}"
      ],
      "metadata": {
        "id": "hdDh59-IldP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWAf5YrgBEQ"
      },
      "outputs": [],
      "source": [
        "# helper function to create dataloader\n",
        "def create_dataloader(dataloader_type):\n",
        "    '''\n",
        "    dataloader_type: train, val, test\n",
        "    '''\n",
        "\n",
        "    clip_sampler_fn_mapper = {\"train\": clip_sampler_random,\n",
        "                            \"val\": clip_sampler_uniform,\n",
        "                            \"test\": clip_sampler_uniform}\n",
        "\n",
        "    kinetics_dataset = create_kinetics_dataset(in_frame_paths_file = os.path.join(FRAME_PATHS_FOLDER, \"{}.tsv\".format(dataloader_type)),\n",
        "                                     in_frame_labels_file = os.path.join(FRAME_LABELS_FOLDER, \"{}_predicted_boxes.csv\".format(dataloader_type)),\n",
        "                                     in_video_path_prefix = VIDEO_PATH_PREFIX,\n",
        "                                     in_clip_sampler = clip_sampler_fn_mapper[dataloader_type],\n",
        "                                     in_video_sampler = torch.utils.data.RandomSampler,\n",
        "                                     in_label_map_file = LABEL_MAP_FILE,\n",
        "                                     in_transform_fn = transform_fn)\n",
        "\n",
        "    #dataloader = torch.utils.data.DataLoader(kinetics_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
        "    dataloader = torch.utils.data.DataLoader(kinetics_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=dict_collate_fn )\n",
        "\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGy7FiawSPcq"
      },
      "outputs": [],
      "source": [
        "# helper function to freeze model weights\n",
        "def freeze_weights(in_model):\n",
        "\n",
        "    assert in_model is not None\n",
        "\n",
        "    for name, param in in_model.named_parameters():\n",
        "        if \"proj\" in name:\n",
        "            continue\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return in_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PFOni3zQIH2"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "def create_model(in_n_classes, model_type=\"c2d\", freeze_body=False):\n",
        "\n",
        "    \"\"\"\n",
        "      in_n_classes (int): The number of classes to be predicted\n",
        "      model_type (str): The type of model to load (either slow or slowfast). Default: c2d, Others: i3d, slow, slowfast\n",
        "    \"\"\"\n",
        "\n",
        "    if model_type not in [\"c2d\",\"i3d\",\"slow\",\"slowfast\"]: raise Exception(\"Please check that type of model is either c2d, i3d, slow or slowfast\")\n",
        "\n",
        "    # input pretrained model\n",
        "    if model_type == \"c2d\":\n",
        "        video_model = c2d_r50(pretrained=True)\n",
        "        model_last_layer = 6\n",
        "    elif model_type == \"i3d\":\n",
        "        video_model = i3d_r50(pretrained=True)\n",
        "        model_last_layer = 6\n",
        "    elif model_type == \"slow\":\n",
        "        video_model = slow_r50(pretrained=True)\n",
        "        model_last_layer = 5\n",
        "    elif model_type == \"slowfast\":\n",
        "        video_model = slowfast_r50(pretrained=True)\n",
        "        model_last_layer = 6\n",
        "\n",
        "    # put model to train mode\n",
        "    video_model = video_model.train()\n",
        "\n",
        "    # freeze body layers\n",
        "    if freeze_body:\n",
        "        video_model = freeze_weights(video_model)\n",
        "\n",
        "    # Change the prediction head to the input number of classes\n",
        "    emb_dim = video_model.blocks[model_last_layer].proj.in_features\n",
        "    video_model.blocks[model_last_layer].proj = torch.nn.Linear(emb_dim, in_n_classes)\n",
        "\n",
        "    # Check if GPU is present, if not use CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Place model on device\n",
        "    video_model = video_model.to(device)\n",
        "\n",
        "    return video_model, device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xYyC5CKWEeG"
      },
      "outputs": [],
      "source": [
        "# helper functions to write and load arguments\n",
        "def write_args(params, output_path):\n",
        "\n",
        "    with open(output_path, \"w\", encoding = \"UTF-8\") as f:\n",
        "        json.dump(params, f, indent=4)\n",
        "\n",
        "def load_args(path):\n",
        "\n",
        "    with open(path, \"r\", encoding = \"UTF-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "737lrMATYQkg"
      },
      "outputs": [],
      "source": [
        "# helper function to get predictions\n",
        "def get_predictions(in_batch, in_model, in_device):\n",
        "    \"\"\"\n",
        "    Supports both:\n",
        "      • new dict form  {\"video\": tensor|list[tensor], \"label\": tensor}\n",
        "      • old tuple form (video_idx, clip_idx, video, label)\n",
        "    Returns\n",
        "    -------\n",
        "    logits, labels\n",
        "    \"\"\"\n",
        "    # ── 1. unpack --------------------------------------------------------\n",
        "    if isinstance(in_batch, dict):                 # new collate\n",
        "        video  = in_batch[\"video\"]\n",
        "        labels = in_batch[\"label\"]\n",
        "    else:                                          # old 4-tuple\n",
        "        _, _, video, labels = in_batch\n",
        "\n",
        "    # ── 2. move video(s) to device --------------------------------------\n",
        "    if MODEL == \"slowfast\":                        # two-path model\n",
        "        # make sure we have a list [slow, fast]\n",
        "        if not isinstance(video, list):\n",
        "            raise ValueError(\"SlowFast expects a list [slow, fast]\")\n",
        "        video = [v.to(in_device, non_blocking=True) for v in video]\n",
        "    else:                                          # single-path model\n",
        "        video = video.to(in_device, non_blocking=True)\n",
        "\n",
        "    # ── 3. labels to device & squeeze -----------------------------------\n",
        "    #labels = labels.to(in_device, non_blocking=True).squeeze()\n",
        "    labels = labels.to(in_device, non_blocking=True).view(-1)\n",
        "\n",
        "    # ── 4. forward ------------------------------------------------------\n",
        "    logits = in_model(video)\n",
        "    return logits, labels\n",
        "\n",
        "# helper function to calculate model loss\n",
        "def get_loss(in_labels_pred, in_labels):\n",
        "\n",
        "    labels_OH = torch.nn.functional.one_hot(in_labels[0],num_classes=N_CLASSES).float()\n",
        "    labels_OH = torch.reshape(labels_OH,(1,-1))\n",
        "\n",
        "    out_loss = torch.nn.functional.cross_entropy(in_labels_pred,labels_OH)\n",
        "\n",
        "    return out_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUjTZIexaH-6"
      },
      "outputs": [],
      "source": [
        "# helper function for validation\n",
        "def validate(in_model, in_dataloader, in_device):\n",
        "\n",
        "    print(\"Validating model...\")\n",
        "\n",
        "    in_model.eval() # To turn off gradient and dropout\n",
        "    total_loss, total_batch = 0, 0\n",
        "    all_labels_pred, all_labels = [],[]\n",
        "    n_iter = int(np.ceil(in_dataloader.dataset.num_clips / in_dataloader.batch_size))\n",
        "\n",
        "    # We do not need to compute gradient since there's no backward pass involved in validation\n",
        "    # torch.no_grad ensures that no gradients are computed and stored\n",
        "    pbar = tqdm(total=n_iter)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in in_dataloader:\n",
        "\n",
        "            labels_pred, labels = get_predictions(batch, in_model, in_device)\n",
        "\n",
        "            loss = get_loss(labels_pred, labels)\n",
        "\n",
        "            # get prediction\n",
        "            labels_pred = torch.nn.functional.softmax(labels_pred,dim=1)\n",
        "            labels_pred = torch.argmax(labels_pred, dim=-1)\n",
        "\n",
        "            all_labels_pred.extend(labels_pred.detach().cpu().numpy().tolist()*labels.shape[0])\n",
        "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batch += 1\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        in_model.train()\n",
        "\n",
        "        avg_loss = total_loss / total_batch\n",
        "        acc = accuracy_score(all_labels, all_labels_pred) * 100\n",
        "        f_score = f1_score(all_labels, all_labels_pred, average = \"macro\") * 100\n",
        "\n",
        "        return avg_loss, acc, f_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcWBi9Aybkkr"
      },
      "outputs": [],
      "source": [
        "# helper function to save model weights\n",
        "def save_model(save_folder, in_model, save_name):\n",
        "\n",
        "    output_path = os.path.join(save_folder, \"{}.pth\".format(save_name))\n",
        "    if hasattr(in_model, \"module\"):\n",
        "        torch.save(in_model.module.state_dict(), output_path)\n",
        "    else:\n",
        "        torch.save(in_model.state_dict(), output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTtliSZMXC5I"
      },
      "outputs": [],
      "source": [
        "def train(in_model, in_optimizer, in_dataloaders, in_model_save_dir, in_log_dir, in_device):\n",
        "\n",
        "    # for data logging\n",
        "    writer = SummaryWriter(log_dir=in_log_dir)\n",
        "    best_loss, best_acc, best_f_score = 1e3, 0, 0\n",
        "\n",
        "    print(\"num_clips:\", in_dataloaders[\"train\"].dataset.num_clips)  # Should print 1\n",
        "    print(\"batch_size:\", in_dataloaders[\"train\"].batch_size)  # Should be an integer\n",
        "\n",
        "    n_iter = int(np.ceil(in_dataloaders[\"train\"].dataset.num_clips / in_dataloaders[\"train\"].batch_size))\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        print('Epoch {} of {}'.format(epoch+1,N_EPOCHS))\n",
        "        for i, batch in tqdm(enumerate(in_dataloaders[\"train\"]), total=n_iter):\n",
        "            #assert isinstance(batch, dict), f\"batch is {type(batch)}\"\n",
        "            # --- inside the training loop, just after you fetch `batch` -------------\n",
        "            if isinstance(batch[\"video\"], list):                     # SlowFast\n",
        "                vid_shapes = [tuple(t.shape) for t in batch[\"video\"]]\n",
        "            else:                                                    # single-path\n",
        "                vid_shapes = tuple(batch[\"video\"].shape)\n",
        "\n",
        "            print(f\"\\nBatch {i}: video {vid_shapes}, label {batch['label'].shape}\")\n",
        "\n",
        "        for i, batch in tqdm(enumerate(in_dataloaders[\"train\"]),total=n_iter):\n",
        "            in_model.train()\n",
        "            in_optimizer.zero_grad()\n",
        "\n",
        "            labels_pred, labels = get_predictions(batch, in_model, in_device)\n",
        "            loss = get_loss(labels_pred, labels)\n",
        "\n",
        "            # Backprop here\n",
        "            loss.backward()\n",
        "            in_optimizer.step()\n",
        "\n",
        "        # Calculate metrics after every epoch\n",
        "        avg_train_loss, train_acc, train_f_score = validate(in_model, in_dataloaders[\"train\"], in_device)\n",
        "        writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
        "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
        "        writer.add_scalar(\"FScore/train\", train_f_score, epoch)\n",
        "\n",
        "        avg_val_loss, val_acc, val_f_score = validate(in_model, in_dataloaders[\"val\"], in_device)\n",
        "        writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
        "        writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "        writer.add_scalar(\"FScore/val\", val_f_score, epoch)\n",
        "\n",
        "        if avg_val_loss <= best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            writer.add_scalar(\"Saved_models/val_loss\", best_loss, epoch)\n",
        "            save_model(in_model_save_dir, in_model, \"best_model_loss\")\n",
        "\n",
        "        if val_acc >= best_acc:\n",
        "            best_acc = val_acc\n",
        "            writer.add_scalar(\"Saved_models/val_acc\", best_acc, epoch)\n",
        "            save_model(in_model_save_dir, in_model, \"best_model_acc\")\n",
        "\n",
        "        if val_f_score >= best_f_score:\n",
        "            best_f_score = val_f_score\n",
        "            writer.add_scalar(\"Saved_models/val_f_score\", best_f_score, epoch)\n",
        "            save_model(in_model_save_dir, in_model, \"best_model_f_score\")\n",
        "\n",
        "    writer.flush()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz8b91lKywLu"
      },
      "outputs": [],
      "source": [
        "def temporal_subsample(in_video_frames,in_labels,num_samples,temporal_dim=-3):\n",
        "    '''\n",
        "    Uniformly subsamples num_samples indices from the temporal dimension of the video.\n",
        "    When num_samples is larger than the size of temporal dimension of the video,\n",
        "    it will sample frames based on nearest neighbor interpolation\n",
        "    Args:\n",
        "        in_video_frames (torch tensor): A video tensor with a temporal dimension\n",
        "        in_labels (list): list of labels correspoinding to each video frame\n",
        "        num_samples (int): The number of equispaced samples to be selected\n",
        "        temporal_dim (int): dimension of temporal to perform temporal subsample\n",
        "    Returns:\n",
        "        Corresponding subsampled temporal outputs\n",
        "    '''\n",
        "\n",
        "    t = in_video_frames.shape[temporal_dim]\n",
        "    assert num_samples > 0 and t > 0\n",
        "\n",
        "    # Sample by nearest neighbor interpolation if num_samples > t\n",
        "    indices = torch.linspace(0,t-1,num_samples)\n",
        "    indices =  torch.clamp(indices,0,t-1).long()\n",
        "\n",
        "    # Carry out sampling\n",
        "    out_video_frames = torch.index_select(in_video_frames,temporal_dim,indices)\n",
        "    out_labels = [in_labels[i] for i in indices.tolist()]\n",
        "\n",
        "    return out_video_frames,out_labels\n",
        "\n",
        "def get_dimensions(in_tensor):\n",
        "    if not in_tensor.ndim >= 2:\n",
        "        raise TypeError(\"Tensor is not a torch image\")\n",
        "    channels = 1 if in_tensor.ndim == 2 else in_tensor.shape[-3]\n",
        "    height, width = in_tensor.shape[-2:]\n",
        "    return [channels, height, width]\n",
        "\n",
        "def Normalize(in_video_frames,in_mean,in_std):\n",
        "    out_video_frames = in_video_frames.permute(1, 0, 2, 3)  # CTHW to TCHW\n",
        "    out_video_frames = normalize(out_video_frames, mean=in_mean, std=in_std)\n",
        "    out_video_frames = out_video_frames.permute(1, 0, 2, 3)  # TCHW to CTHW\n",
        "\n",
        "    return out_video_frames\n",
        "\n",
        "def RandomShortSideScale(in_video_frames,in_min_size,in_max_size):\n",
        "    size = torch.randint(in_min_size, in_max_size + 1, (1,)).item()\n",
        "    return short_side_scale(in_video_frames, size)\n",
        "\n",
        "def RandomCrop(in_video_frames,in_crop_size):\n",
        "    size = tuple((int(in_crop_size),int(in_crop_size)))\n",
        "\n",
        "    _, h, w = get_dimensions(in_video_frames)\n",
        "    th, tw = size\n",
        "\n",
        "    if h < th or w < tw:\n",
        "        raise ValueError(f\"Required crop size {(th, tw)} is larger than input image size {(h, w)}\")\n",
        "\n",
        "    if w == tw and h == th:\n",
        "        i = 0\n",
        "        j = 0\n",
        "    else:\n",
        "        i = torch.randint(0, h - th + 1, size=(1,)).item()\n",
        "        j = torch.randint(0, w - tw + 1, size=(1,)).item()\n",
        "\n",
        "    return crop(in_video_frames, i, j, th, tw)\n",
        "\n",
        "def RandomHorizontalFlip(in_video_frames,in_p=0.5):\n",
        "    if torch.rand(1) < in_p:\n",
        "        return hflip(in_video_frames)\n",
        "\n",
        "    return in_video_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ujOXIuTl-WO"
      },
      "outputs": [],
      "source": [
        "# helper function for getting model predictions\n",
        "def obtain_model_predictions(in_model, in_dataloader, in_device):\n",
        "\n",
        "    print(\"Obtaining predictions...\")\n",
        "\n",
        "    in_model.eval() # To turn off gradient and dropout\n",
        "    out_all_labels_pred, out_all_labels = [],[]\n",
        "    n_iter = int(np.ceil(in_dataloader.dataset.num_clips / in_dataloader.batch_size))\n",
        "\n",
        "    # We do not need to compute gradient since there's no backward pass involved in validation\n",
        "    # torch.no_grad ensures that no gradients are computed and stored\n",
        "    pbar = tqdm(total=n_iter)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in in_dataloader:\n",
        "\n",
        "            labels_pred, labels = get_predictions(batch, in_model, in_device)\n",
        "\n",
        "            labels_pred = torch.nn.functional.softmax(labels_pred,dim=1)\n",
        "            labels_pred = torch.argmax(labels_pred, dim=-1)\n",
        "\n",
        "            out_all_labels_pred.extend(labels_pred.detach().cpu().numpy().tolist()*labels.shape[0])\n",
        "            out_all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        in_model.train()\n",
        "\n",
        "    # store relevant outputs as df\n",
        "    assert len(out_all_labels) == len(out_all_labels_pred)\n",
        "\n",
        "    out_df_model_preds = pd.DataFrame()\n",
        "    out_df_model_preds['label'] = out_all_labels\n",
        "    out_df_model_preds['prediction'] = out_all_labels_pred\n",
        "\n",
        "    out_df_model_preds['result'] = ''\n",
        "    for i, row_i in out_df_model_preds.iterrows():\n",
        "        if row_i['label'] == row_i['prediction']:\n",
        "            out_df_model_preds.at[i,'result'] = 'correct'\n",
        "        else:\n",
        "            out_df_model_preds.at[i,'result'] = 'incorrect'\n",
        "\n",
        "    return out_df_model_preds\n",
        "\n",
        "def obtain_other_model_predictions(in_model, in_dataloader, in_device, in_other_to_model_id_dict):\n",
        "\n",
        "    print(\"Obtaining predictions...\")\n",
        "\n",
        "    in_model.eval() # To turn off gradient and dropout\n",
        "    out_all_labels_pred, out_all_labels = [],[]\n",
        "    n_iter = int(np.ceil(in_dataloader.dataset.num_clips / in_dataloader.batch_size))\n",
        "\n",
        "    # We do not need to compute gradient since there's no backward pass involved in validation\n",
        "    # torch.no_grad ensures that no gradients are computed and stored\n",
        "    pbar = tqdm(total=n_iter)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in in_dataloader:\n",
        "\n",
        "            labels_pred, labels = get_predictions(batch, in_model, in_device)\n",
        "\n",
        "            labels_pred = torch.nn.functional.softmax(labels_pred,dim=1)\n",
        "            labels_pred = torch.argmax(labels_pred, dim=-1)\n",
        "\n",
        "            out_all_labels_pred.extend(labels_pred.detach().cpu().numpy().tolist()*labels.shape[0])\n",
        "            out_all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        in_model.train()\n",
        "\n",
        "    # store relevant outputs as df\n",
        "    assert len(out_all_labels) == len(out_all_labels_pred)\n",
        "\n",
        "    out_df_model_preds = pd.DataFrame()\n",
        "    out_df_model_preds['label'] = out_all_labels\n",
        "    out_df_model_preds['label'] = out_df_model_preds['label'].map(in_other_to_model_id_dict)\n",
        "    out_df_model_preds['prediction'] = out_all_labels_pred\n",
        "\n",
        "    out_df_model_preds['result'] = ''\n",
        "    for i, row_i in out_df_model_preds.iterrows():\n",
        "        if row_i['label'] == row_i['prediction']:\n",
        "            out_df_model_preds.at[i,'result'] = 'correct'\n",
        "        else:\n",
        "            out_df_model_preds.at[i,'result'] = 'incorrect'\n",
        "\n",
        "    return out_df_model_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwCFIiiImd4A"
      },
      "outputs": [],
      "source": [
        "def per_class_accuracy(in_df_results,in_target_class):\n",
        "\n",
        "    target_classes = sorted(list(set(in_df_results[in_target_class])))\n",
        "\n",
        "    out_class_accuracy = pd.DataFrame(columns=[in_target_class]+['total','correct','incorrect','per_correct','per_incorrect'])\n",
        "\n",
        "    for target_class in target_classes:\n",
        "        df_class = in_df_results[in_df_results[in_target_class]==target_class]\n",
        "        total = len(df_class)\n",
        "\n",
        "        df_correct = df_class[df_class['result']=='correct']\n",
        "        correct = len(df_correct)\n",
        "\n",
        "        df_incorrect = df_class[df_class['result']=='incorrect']\n",
        "        incorrect = len(df_incorrect)\n",
        "\n",
        "        assert correct + incorrect == total\n",
        "\n",
        "        per_correct = '{} %'.format(round(correct/total*100,2))\n",
        "        per_incorrect = '{} %'.format(round(incorrect/total*100,2))\n",
        "\n",
        "        # store results\n",
        "        df_dict = pd.DataFrame.from_dict({in_target_class:[target_class],\n",
        "                                          'total':[total],\n",
        "                                          'correct':[correct],\n",
        "                                          'incorrect':[incorrect],\n",
        "                                          'per_correct':[per_correct],\n",
        "                                          'per_incorrect':[per_incorrect]})\n",
        "        out_class_accuracy = pd.concat([out_class_accuracy,df_dict],ignore_index=True)\n",
        "\n",
        "    # get action\n",
        "    out_class_accuracy['action'] = out_class_accuracy[in_target_class].map(ACTION_ID_DICT)\n",
        "    out_class_accuracy = out_class_accuracy[[in_target_class,'action','total','correct','incorrect','per_correct','per_incorrect']]\n",
        "\n",
        "    # determine overall\n",
        "    total_sum = out_class_accuracy['total'].sum()\n",
        "    correct_sum = out_class_accuracy['correct'].sum()\n",
        "    incorrect_sum = out_class_accuracy['incorrect'].sum()\n",
        "\n",
        "    assert correct_sum + incorrect_sum == total_sum\n",
        "\n",
        "    per_correct_sum = '{} %'.format(round(correct_sum/total_sum*100,2))\n",
        "    per_incorrect_sum = '{} %'.format(round(incorrect_sum/total_sum*100,2))\n",
        "\n",
        "    df_dict = pd.DataFrame.from_dict({in_target_class:['Overall'],\n",
        "                                      'action':['Overall'],\n",
        "                                      'total':[total_sum],\n",
        "                                      'correct':[correct_sum],\n",
        "                                      'incorrect':[incorrect_sum],\n",
        "                                      'per_correct':[per_correct_sum],\n",
        "                                      'per_incorrect':[per_incorrect_sum]})\n",
        "    out_class_accuracy = pd.concat([out_class_accuracy,df_dict],ignore_index=True)\n",
        "\n",
        "    return out_class_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3oL4fw6nJEa"
      },
      "outputs": [],
      "source": [
        "def generate_confusion_data(in_df_match,in_label_name,in_pred_name,remove_correct=False):\n",
        "\n",
        "    # determine unique class\n",
        "    unique_classes = sorted(list(set(in_df_match[in_label_name])))\n",
        "    unique_classes += sorted(list(set(in_df_match[in_pred_name])))\n",
        "    unique_classes = sorted(list(set(unique_classes)))\n",
        "\n",
        "    # obtain data for confusion matrix\n",
        "    out_df_confusion_data = pd.DataFrame(columns=['label','prediction','number','percentage'])\n",
        "\n",
        "    for unique_label in unique_classes:\n",
        "\n",
        "        df_label = in_df_match.loc[in_df_match[in_label_name]==unique_label]\n",
        "        label_total = float(len(df_label))\n",
        "\n",
        "        if label_total != 0:\n",
        "\n",
        "            for unique_pred in unique_classes:\n",
        "\n",
        "                df_pred = df_label.loc[df_label[in_pred_name]==unique_pred]\n",
        "                pred_total = float(len(df_pred))\n",
        "\n",
        "                # remove correct predictions\n",
        "                if remove_correct:\n",
        "                    if unique_label == unique_pred:\n",
        "                        pred_total = 0\n",
        "\n",
        "                # calculate prediction percentage\n",
        "                pred_percent = round(pred_total/label_total*100,2)\n",
        "\n",
        "                # store\n",
        "                df_dict = pd.DataFrame.from_dict({'label':[unique_label],\n",
        "                                                'prediction':[unique_pred],\n",
        "                                                'number':[pred_total],\n",
        "                                                'percentage':[pred_percent]})\n",
        "\n",
        "                out_df_confusion_data = pd.concat([out_df_confusion_data,df_dict],ignore_index=True)\n",
        "\n",
        "    return out_df_confusion_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Y7H51KNY4q"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr9wnX9ANeNy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# path variables\n",
        "pri_dir = '/content/MyDrive/MyDrive/autosynthda/action-recognition/'\n",
        "raw_data_dir = os.path.join(pri_dir, 'raw_data')\n",
        "checkpoint_dir = os.path.join(pri_dir, 'checkpoint')\n",
        "\n",
        "training_data_dir = os.path.join(pri_dir, 'training_data')\n",
        "FRAMES_DIR = os.path.join(training_data_dir, 'frames')\n",
        "FRAME_PATHS_FOLDER = os.path.join(training_data_dir, 'frame_lists')\n",
        "FRAME_LABELS_FOLDER = os.path.join(training_data_dir, 'annotations')\n",
        "LABEL_MAP_FILE = os.path.join(FRAME_LABELS_FOLDER,\"action_list.pbtxt\")\n",
        "VIDEO_PATH_PREFIX = FRAMES_DIR + '/'\n",
        "\n",
        "# Print directory paths\n",
        "print(\"Raw Videos Directory:\", raw_data_dir)\n",
        "print(\"Frames Directory:\", FRAMES_DIR)\n",
        "print(\"Annotations Directory:\", FRAME_LABELS_FOLDER)\n",
        "print(\"Training Data Directory:\", training_data_dir)\n",
        "print(\"Checkpoint Directory:\", checkpoint_dir)\n",
        "\n",
        "!ls /content/MyDrive/MyDrive/autosynthda/action-recognition/\n",
        "pri_dir      = Path(\"/content/MyDrive/MyDrive/autosynthda/action-recognition\")\n",
        "raw_data_dir = pri_dir / \"raw_data\"\n",
        "\n",
        "for p in (pri_dir, raw_data_dir):\n",
        "    p = Path(p)                       # ← guarantees a Path instance\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"{p} does not exist.\")\n",
        "    if not p.is_dir():\n",
        "        raise NotADirectoryError(f\"{p} exists but is not a directory.\")\n",
        "\n",
        "pri_dir      = Path(\"/content/MyDrive/MyDrive/autosynthda/action-recognition\")\n",
        "raw_data_dir = pri_dir / \"raw_data\"            # this folder has train/val/test inside\n",
        "FOLDS        = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "print(\"Root project dir :\", pri_dir)\n",
        "print(\"Raw videos dir   :\", raw_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlKZN4ZjOn8F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model variables\n",
        "DATA_TYPE = \"kinetics\"\n",
        "DATASET_NAME = \"kinetics\"\n",
        "MODEL = \"slowfast\" #c2d, i3d, slow, slowfast\n",
        "FREEZE_BODY = True\n",
        "\n",
        "# transform variables\n",
        "MIN_SIDE_SIZE = 256\n",
        "MAX_SIDE_SIZE = 320\n",
        "CROP_SIZE = 244\n",
        "MEAN = [0.45, 0.45, 0.45]\n",
        "STD = [0.225, 0.225, 0.225]\n",
        "\n",
        "SLOWFAST_ALPHA = 4\n",
        "if MODEL in [\"c2d\",\"i3d\",\"slow\"]:\n",
        "    NUM_FRAMES = 16\n",
        "elif MODEL == \"slowfast\":\n",
        "    NUM_FRAMES = 32\n",
        "CLIP_SAMPLING_RATE = 1\n",
        "CLIP_DURATION = (NUM_FRAMES * CLIP_SAMPLING_RATE)\n",
        "\n",
        "# FPS = 30\n",
        "CLIP_DURATION = NUM_FRAMES # clip duration is in terms of num of frames\n",
        "\n",
        "N_CLASSES = 2 # Change the number of classes here\n",
        "\n",
        "# dataloader variables\n",
        "NUM_WORKERS = 0\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# training variables\n",
        "N_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr7lBvLZRmuJ"
      },
      "outputs": [],
      "source": [
        "# set the random seed\n",
        "RANDOM_SEED = 17\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UClQXF5TSzKb"
      },
      "source": [
        "# Prep Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdAxqNeybKNg"
      },
      "source": [
        "## Define Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4SgpN9VS1yF"
      },
      "outputs": [],
      "source": [
        "# define action type\n",
        "action_type_dict = {'adl':'human',\n",
        "                    'fall':'human'\n",
        "                    }\n",
        "ACTION_ID_DICT = {0:'adl',1:'fall'}\n",
        "\n",
        "\n",
        "# create a new empty action_filepath\n",
        "action_filepath = os.path.join(FRAME_LABELS_FOLDER,'action_list.pbtxt')\n",
        "if os.path.exists(action_filepath):\n",
        "  os.remove(action_filepath)\n",
        "with open(action_filepath, 'w') as file:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "# write\n",
        "for action_name, action_type in action_type_dict.items():\n",
        "  action_id = list(action_type_dict).index(action_name)\n",
        "  write_action(action_filepath,action_name,action_id,action_type,tabsize=2)\n",
        "\n",
        "# check\n",
        "with open(action_filepath, 'r') as action_file:\n",
        "    content = action_file.read()\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGeW1CChnkGI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBuEA_LwbNiP"
      },
      "source": [
        "## Obtain Videos [Random Split]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slOwfCJ2XcQq"
      },
      "outputs": [],
      "source": [
        "# obtain video information\n",
        "#df_videoinfo = pd.DataFrame()\n",
        "#vid_id = 0\n",
        "\n",
        "#action_names = glob.glob(os.path.join(raw_data_dir,\"*\"))\n",
        "#action_names = [os.path.basename(action_name) for action_name in action_names]\n",
        "\n",
        "#for action_name in action_names:\n",
        " # action_dir = os.path.join(raw_data_dir,action_name)\n",
        " # video_paths = glob.glob(os.path.join(action_dir,\"*\"))\n",
        " # for video_path in video_paths:\n",
        " #   videoinfo = get_videoinfo(video_path)\n",
        " #   videoinfo['action_name'] = action_name\n",
        " #   videoinfo['video_path'] = video_path\n",
        " #   videoinfo['vid_id'] = vid_id\n",
        " #   videoinfo['action_id'] = list(action_type_dict).index(action_name)\n",
        "\n",
        "  #  df_dict = pd.DataFrame.from_dict([videoinfo])\n",
        "  #  df_videoinfo = pd.concat([df_videoinfo,df_dict],ignore_index=True)\n",
        "\n",
        "  #  vid_id += 1\n",
        "\n",
        "# rearrange columns\n",
        "#df_videoinfo = df_videoinfo[['vid_id','video_path','fps','no_images','frameSize','action_name','action_id']]\n",
        "#df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJTEErmua6Oj"
      },
      "outputs": [],
      "source": [
        "# remove videos that has fewer frames that FRAMES_THRES\n",
        "#df_videoinfo = df_videoinfo.loc[df_videoinfo['no_images']>NUM_FRAMES]\n",
        "#df_videoinfo.reset_index(inplace=True,drop=True)\n",
        "#df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rex6aNA5Y_Vj"
      },
      "outputs": [],
      "source": [
        "# check number of videos\n",
        "#for action_name in action_names:\n",
        "  #num_vids = len(df_videoinfo[df_videoinfo['action_name']==action_name])\n",
        "  #print(action_name,num_vids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCr7VKsKbFxr"
      },
      "source": [
        "## Create Train Val Test Split [Random Split]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPGQmW4gVLBz"
      },
      "outputs": [],
      "source": [
        "# train val test split\n",
        "#split_df, X_train, X_test = split(df_videoinfo,'action_id',0.2,'train','test')\n",
        "#split_df, X_train, X_val = split(X_train,'action_id',0.2,'train','val')\n",
        "\n",
        "# create new df_videoinfo\n",
        "#df_videoinfo = pd.concat([X_train,X_val,X_test],ignore_index=True)\n",
        "#df_videoinfo = df_videoinfo[['vid_id','video_path','fps','no_images','frameSize','action_name','action_id','fold']]\n",
        "\n",
        "#df_videoinfo.sort_values(by=['vid_id'],axis=0,inplace=True)\n",
        "#df_videoinfo.reset_index(inplace=True,drop=True)\n",
        "#df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZP_wObVVfGO"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "#df_videoinfo.to_csv(os.path.join(training_data_dir,'video_info.csv'),index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uJczSjQc6Zz"
      },
      "source": [
        "## Obtain Videos [Fixed Split for AutoSynthDa]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "ACTION_NAME_TO_ID = {\"adl\": 0, \"fall\": 1}\n",
        "\n",
        "rows, vid_id = [], 0\n",
        "for fold in FOLDS:\n",
        "    for action_dir in (raw_data_dir / fold).iterdir():\n",
        "        if not action_dir.is_dir():\n",
        "            continue\n",
        "        action_name = action_dir.name\n",
        "        for video_path in action_dir.glob(\"*\"):\n",
        "            info = get_videoinfo(str(video_path))\n",
        "            info.update({\n",
        "                \"vid_id\"     : vid_id,\n",
        "                \"video_path\" : str(video_path),   # ← contains the fold\n",
        "                \"action_name\": action_name,\n",
        "                \"action_id\"  : ACTION_NAME_TO_ID[action_name],\n",
        "                \"fold\"       : fold\n",
        "            })\n",
        "            rows.append(info)\n",
        "            vid_id += 1\n",
        "\n",
        "\n",
        "\n",
        "#df_videoinfo = df_videoinfo[['vid_id','video_path','fps','no_images','frameSize','action_name','action_id']]"
      ],
      "metadata": {
        "id": "vZ15ytO31F5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train Val Test Split [Fixed Split for AutoSynthDa]"
      ],
      "metadata": {
        "id": "lZ_PCjpU0i3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_videoinfo = (pd.DataFrame(df_rows)\n",
        "                  #.loc[lambda d: d[\"no_images\"] > NUM_FRAMES]   # keep long enough\n",
        "                  #.reset_index(drop=True)\n",
        "                  #.loc[:, ['vid_id', 'video_path', 'fps', 'no_images',\n",
        "                          #'frameSize', 'action_name', 'action_id', 'fold']])\n",
        "\n",
        "FOLDS = [\"train\", \"val\", \"test\"]          # the three constant splits\n",
        "\n",
        "# ── 1.  keep only videos with enough frames, *per fold* ────────────────\n",
        "df_videoinfo = (pd.DataFrame(rows)\n",
        "                  .loc[lambda d: d[\"no_images\"] > NUM_FRAMES]\n",
        "                  .reset_index(drop=True))\n",
        "\n",
        "pri_dir            = Path(\"/content/MyDrive/MyDrive/autosynthda/action-recognition\")\n",
        "training_data_dir  = pri_dir / \"training_data\"         # ← now a Path object\n",
        "\n",
        "df_videoinfo.to_csv(training_data_dir / \"video_info.csv\", index=False)\n",
        "\n",
        "for fold in FOLDS:\n",
        "    print(df_videoinfo.query(\"fold == @fold\")[\"video_path\"].head().tolist())"
      ],
      "metadata": {
        "id": "1HeX4dpH1Eho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#action_names = glob.glob(os.path.join(raw_data_dir,\"*\"))\n",
        "#action_names = [os.path.basename(action_name) for action_name in action_names]\n",
        "#for action_name in action_names:\n",
        "  #num_vids = len(df_videoinfo[df_videoinfo['action_name']==action_name])\n",
        "  #print(action_name,num_vids)\n",
        "\n",
        "\n",
        "df_videoinfo\n",
        "#print(df_videoinfo[\"video_path\"].head().tolist())\n"
      ],
      "metadata": {
        "id": "M3lDdDUd9Wvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count videos and double check folders"
      ],
      "metadata": {
        "id": "YspkQdR7MmEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = (\n",
        "    df_videoinfo\n",
        "      .groupby([\"action_name\", \"fold\"])   # multi-index: class & split\n",
        "      .size()                             # how many rows (i.e. videos)\n",
        "      .unstack(fill_value=0)              # columns → train / val / test\n",
        "      .assign(total=lambda d: d.sum(1))   # optional total per class\n",
        "      .sort_index()                       # alphabetical by class name\n",
        ")\n",
        "\n",
        "print(summary)        # nice table view"
      ],
      "metadata": {
        "id": "qE2_KP8bMqsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Frames_dir [all]"
      ],
      "metadata": {
        "id": "9eayGf5X0qZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI7jTJgqVqRZ"
      },
      "outputs": [],
      "source": [
        "# read video info\n",
        "print(training_data_dir)\n",
        "df_videoinfo = pd.read_csv(os.path.join(training_data_dir,'video_info.csv'))\n",
        "\n",
        "print(df_videoinfo[\"video_path\"].head().tolist())\n",
        "df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "bad = [p for p in df_videoinfo[\"video_path\"] if not Path(p).exists()]\n",
        "print(f\"{len(bad)=}\")\n",
        "print(bad[:5])"
      ],
      "metadata": {
        "id": "Le8Q0YmQBu1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4EB3vg5c-G8"
      },
      "outputs": [],
      "source": [
        "df_videoinfo\n",
        "df_frame_info = create_frames_dir(df_videoinfo,FRAMES_DIR)\n",
        "df_frame_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfbg7QiTU17p"
      },
      "outputs": [],
      "source": [
        "df_frame_info.to_csv(os.path.join(training_data_dir,'frame_info.csv'),index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRr_Xm5VFtN"
      },
      "source": [
        "## Prep frame_lists_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LYmkcMLVlj1"
      },
      "outputs": [],
      "source": [
        "# read video info\n",
        "df_videoinfo = pd.read_csv(os.path.join(training_data_dir,'video_info.csv'))\n",
        "df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw_Gx7f0Vv_S"
      },
      "outputs": [],
      "source": [
        "# create train.tsv\n",
        "train_df = df_videoinfo[df_videoinfo['fold']=='train']\n",
        "df_train = create_frame_lists_csv(train_df)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsfNbclzYGah"
      },
      "outputs": [],
      "source": [
        "# create val.tsv\n",
        "val = df_videoinfo[df_videoinfo['fold']=='val']\n",
        "df_val = create_frame_lists_csv(val)\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDlvFkWsYICL"
      },
      "outputs": [],
      "source": [
        "# create test.tsv\n",
        "test = df_videoinfo[df_videoinfo['fold']=='test']\n",
        "df_test = create_frame_lists_csv(test)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwbrm5MrYNuS"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "df_train.to_csv(os.path.join(FRAME_PATHS_FOLDER,'train.tsv'),index=0,header=False,sep=\"\\t\")\n",
        "df_val.to_csv(os.path.join(FRAME_PATHS_FOLDER,'val.tsv'),index=0,header=False,sep=\"\\t\")\n",
        "df_test.to_csv(os.path.join(FRAME_PATHS_FOLDER,'test.tsv'),index=0,header=False,sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4tqGLLeYfoX"
      },
      "source": [
        "## Prep annotations_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnL-qIIBYhrq"
      },
      "outputs": [],
      "source": [
        "# import df_video_info\n",
        "df_videoinfo = pd.read_csv(os.path.join(training_data_dir,'video_info.csv'))\n",
        "print(df_videoinfo[\"video_path\"].head().tolist())\n",
        "df_videoinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWPI01W6ZKYM"
      },
      "outputs": [],
      "source": [
        "# train_predicted_boxes.csv\n",
        "train_df = df_videoinfo[df_videoinfo['fold']=='train']\n",
        "df_train = create_annotations_csv(train_df)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeD-063BcWGI"
      },
      "outputs": [],
      "source": [
        "# val_predicted_boxes.csv\n",
        "val = df_videoinfo[df_videoinfo['fold']=='val']\n",
        "df_val = create_annotations_csv(val)\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tMDJeI6cbBK"
      },
      "outputs": [],
      "source": [
        "# test_predicted_boxes.csv\n",
        "test = df_videoinfo[df_videoinfo['fold']=='test']\n",
        "df_test = create_annotations_csv(test)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T7Us81dcePO"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "df_train.to_csv(os.path.join(FRAME_LABELS_FOLDER,'train_predicted_boxes.csv'),index=0,header=False)\n",
        "df_val.to_csv(os.path.join(FRAME_LABELS_FOLDER,'val_predicted_boxes.csv'),index=0,header=False)\n",
        "df_test.to_csv(os.path.join(FRAME_LABELS_FOLDER,'test_predicted_boxes.csv'),index=0,header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoqnGv3ENZ39"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOxMqMfIf3JJ"
      },
      "source": [
        "## Create Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SHYhaCif1zs"
      },
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "train_dataloader = create_dataloader(\"train\")\n",
        "val_dataloader = create_dataloader(\"val\")\n",
        "\n",
        "current_dataloaders = {\"train\": train_dataloader,\n",
        "                      \"val\": val_dataloader}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQBnBd1on7eo"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKnafrB0QBLp"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "current_model, current_device = create_model(N_CLASSES, model_type=MODEL, freeze_body=FREEZE_BODY)\n",
        "\n",
        "# obtain optimizer and loss\n",
        "current_optimizer = torch.optim.Adam(current_model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unit tests for loaders\n",
        "def _test_loader(dl, name=\"\"):\n",
        "    batch = next(iter(dl))\n",
        "\n",
        "    # Base checks\n",
        "    assert isinstance(batch, dict), f\"{name}: expected dict, got {type(batch)}\"\n",
        "    assert \"video\" in batch and \"label\" in batch, f\"{name}: missing keys {batch.keys()}\"\n",
        "\n",
        "    video = batch[\"video\"]\n",
        "    label = batch[\"label\"]\n",
        "\n",
        "    # Handle single-path vs SlowFast (2-path)\n",
        "    if isinstance(video, list):\n",
        "        assert len(video) == 2, f\"{name}: expected 2 tensors for SlowFast, got {len(video)}\"\n",
        "        slow, fast = video\n",
        "        assert slow.dtype == torch.float32 and fast.dtype == torch.float32, f\"{name}: video dtype not float32\"\n",
        "        vid_shape = [tuple(slow.shape), tuple(fast.shape)]\n",
        "    else:\n",
        "        assert video.dtype == torch.float32, f\"{name}: video dtype not float32\"\n",
        "        vid_shape = tuple(video.shape)\n",
        "\n",
        "    assert label.dtype in (torch.int64, torch.long), f\"{name}: label dtype not int64/long\"\n",
        "\n",
        "    print(f\"✓ {name} loader OK — video {vid_shape}, label {label.shape}\")\n",
        "\n",
        "_test_loader(train_dataloader, \"train\")\n",
        "_test_loader(val_dataloader,   \"val\")"
      ],
      "metadata": {
        "id": "oau_g_9vjoVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check what is inside each loader (debugging)\n",
        "\n",
        "import torch, inspect\n",
        "\n",
        "sample_batch = next(iter(current_dataloaders[\"train\"]))\n",
        "\n",
        "print(\"type(batch) :\", type(sample_batch))\n",
        "print(\"tuple length:\", len(sample_batch))\n",
        "\n",
        "for idx, item in enumerate(sample_batch):\n",
        "    if torch.is_tensor(item):\n",
        "        print(f\"[{idx}] tensor  shape={tuple(item.shape)}  dtype={item.dtype}\")\n",
        "    else:\n",
        "        print(f\"[{idx}] {type(item)} → {item if len(str(item)) < 100 else '...'}\")"
      ],
      "metadata": {
        "id": "1VIo34-rj90p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kfDMe55n85Z"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjHVy5PlWYau"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# create output directory\n",
        "current_date_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
        "current_output_dir = os.path.join(checkpoint_dir, \"{}\".format(current_date_time))\n",
        "os.makedirs(current_output_dir,exist_ok=True)\n",
        "model_save_dir = os.path.join(current_output_dir, \"saved_models\")\n",
        "create_dir(model_save_dir)\n",
        "model_log_dir = os.path.join(current_output_dir, \"logs\")\n",
        "create_dir(model_log_dir)\n",
        "model_evaluation_dir = os.path.join(current_output_dir, \"model_evaluation\")\n",
        "create_dir(model_evaluation_dir)\n",
        "args_file = os.path.join(current_output_dir, \"args.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-t4EBKsWsZZ"
      },
      "outputs": [],
      "source": [
        "# save current parameters in a json for easier reference in the future\n",
        "params = {\"args_file\": args_file, \"current_date_time\": current_date_time, \"random_seed\": RANDOM_SEED,\n",
        "          \"current_output_dir\": current_output_dir, \"model_save_dir\": model_save_dir, \"model_log_dir\": model_log_dir, \"model_evaluation_dir\": model_evaluation_dir,\n",
        "          \"frame_paths_folder\": FRAME_PATHS_FOLDER, \"frame_labels_folder\": FRAME_LABELS_FOLDER, \"label_map_file\": LABEL_MAP_FILE, \"video_path_prefix\": VIDEO_PATH_PREFIX,\n",
        "          \"data_type\": DATA_TYPE, \"dataset_name\": DATASET_NAME, \"model\": MODEL, \"freeze_body\": FREEZE_BODY, \"min_side_size\": MIN_SIDE_SIZE,  \"max_side_size\": MAX_SIDE_SIZE, \"crop_size\": CROP_SIZE, \"mean\": MEAN, \"std\": STD,\n",
        "          \"num_frames\": NUM_FRAMES, \"clip_sampling_rate\": CLIP_SAMPLING_RATE, \"clip_duration\": CLIP_DURATION,\n",
        "          \"n_classes\": N_CLASSES,\n",
        "          \"num_workers\": NUM_WORKERS, \"batch_size\": BATCH_SIZE,\n",
        "          \"n_epochs\" : N_EPOCHS, \"learning_rate\": LEARNING_RATE}\n",
        "\n",
        "write_args(params, args_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAn20umkyDK_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.distributed as dist\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"RANK\"] = \"0\"  # Rank of this process\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"  # Total number of processes\n",
        "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"  # Master node IP (localhost for single-node)\n",
        "os.environ[\"MASTER_PORT\"] = \"29500\"  # Any free port\n",
        "\n",
        "# Initialize process group\n",
        "dist.init_process_group(backend=\"nccl\")  # Change to \"gloo\" for CPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the number of clips being more than the number of videos u passed in is normal\n",
        "# Bottom line: the bigger number is normal—the dataset treats every\n",
        "# (start-time, end-time) pair as a separate sample so you can squeeze multiple\n",
        "# training steps out of one long video."
      ],
      "metadata": {
        "id": "-zFZwBNCuASi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUeebOP0Wxph"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "train(current_model, current_optimizer, current_dataloaders, model_save_dir, model_log_dir, current_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eCpNUhONbdn"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVUwQuj8oBI8"
      },
      "source": [
        "## Create Test Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsqrTqPDoDEN"
      },
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "test_dataloader = create_dataloader(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjJZ7gCqFeP"
      },
      "source": [
        "## Loading Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebB40e82mMXX"
      },
      "outputs": [],
      "source": [
        "evaluation_target = 'test'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "eoPWYk4HxTZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVU9dWJXkDgI"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "current_model, current_device = create_model(N_CLASSES, model_type=MODEL, freeze_body=FREEZE_BODY)\n",
        "model_weights = torch.load(\"/content/MyDrive/MyDrive/autosynthda/action-recognition/checkpoint/06-22-2025-12-52-04/saved_models/best_model_acc.pth\")\n",
        "current_model.load_state_dict(model_weights)\n",
        "current_model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4XVvSjroIC3"
      },
      "source": [
        "## Obtain Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3Dt0YkrkHlq"
      },
      "outputs": [],
      "source": [
        "# obtain model predictions\n",
        "df_model_preds = obtain_model_predictions(current_model, test_dataloader, current_device)\n",
        "# save\n",
        "df_model_preds.to_csv(os.path.join(model_evaluation_dir, \"predictions_{}.csv\").format(evaluation_target),index=0)\n",
        "\n",
        "df_model_preds\n",
        "print(df_model_preds.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Point to the CSV you saved from the best-accuracy checkpoint\n",
        "csv_path = \"/content/MyDrive/MyDrive/autosynthda/action-recognition/checkpoint/06-22-2025-12-52-04/model_evaluation/predictions_test.csv\"\n",
        "# e.g. \"/content/drive/.../predictions_test_acc.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# accuracy  = fraction of rows where prediction matches the ground-truth label\n",
        "accuracy = (df[\"label\"] == df[\"prediction\"]).mean()\n",
        "\n",
        "# 0-1 loss  = 1 − accuracy\n",
        "zero_one_loss = 1 - accuracy\n",
        "\n",
        "print(f\"Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"misclassifying loss: {zero_one_loss:.4f}\")"
      ],
      "metadata": {
        "id": "eTvFoTS_3pkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQzFXR7CoMio"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BiSKYwKkH5k"
      },
      "outputs": [],
      "source": [
        "# calculate per class accuracy\n",
        "df_class_accuracy = per_class_accuracy(df_model_preds,'label')\n",
        "\n",
        "# save\n",
        "df_class_accuracy.to_csv(os.path.join(model_evaluation_dir, \"accuracy_{}.csv\").format(evaluation_target),index=0)\n",
        "\n",
        "df_class_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq1FsxKwmVS_"
      },
      "outputs": [],
      "source": [
        "# generate confusion matrix\n",
        "df_confusion_data = generate_confusion_data(df_model_preds,'label','prediction',remove_correct=True)\n",
        "df_heatmap = df_confusion_data.pivot(index='label',columns='prediction',values='percentage')\n",
        "\n",
        "# save\n",
        "df_heatmap.to_csv(os.path.join(model_evaluation_dir, \"confusion_{}.csv\").format(evaluation_target),index=0)\n",
        "\n",
        "df_heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output to .txt file for export to main AutoSynthDa pipeline"
      ],
      "metadata": {
        "id": "HS6mYLz69IA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------\n",
        "# 1) user-defined “weight” to embed in the file names\n",
        "# --------------------------------------------------------------------\n",
        "weight_val = 0.4                       # <= set this to 0.2, 0.3, … as needed\n",
        "print(\"Columns in df_class_accuracy:\", df_class_accuracy.columns.tolist())\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2) where to save them\n",
        "# --------------------------------------------------------------------\n",
        "txt_dir = '/content/MyDrive/MyDrive/autosynthda/action-recognition/results/'        # or any other folder you prefer\n",
        "#txt_dir = \"/content/MyDrive/MyDrive/autosynthda/action-recognition/results\"\n",
        "\n",
        "timestamp          = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
        "current_output_dir = os.path.join(txt_dir, timestamp)\n",
        "\n",
        "os.makedirs(current_output_dir, exist_ok=True)   # <-- THIS was missing\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2.  File paths\n",
        "# --------------------------------------------------------------------\n",
        "acc_txt_path  = os.path.join(current_output_dir, f\"acc_{weight_val}.txt\")\n",
        "loss_txt_path = os.path.join(current_output_dir, f\"loss_{weight_val}.txt\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3.  Write per-class accuracies\n",
        "# --------------------------------------------------------------------\n",
        "with open(acc_txt_path, \"w\") as f:\n",
        "    for _, row in df_class_accuracy.iterrows():\n",
        "        cls = row[\"action\"]\n",
        "\n",
        "        acc_str = str(row[\"per_correct\"]).strip()\n",
        "        acc_val = (float(acc_str.rstrip(\"%\").strip()) / 100.0\n",
        "                   if acc_str.endswith(\"%\") else float(acc_str))\n",
        "\n",
        "        f.write(f\"acc_{weight_val}_{cls} = {acc_val:.4f}\\n\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 4.  Write overall loss\n",
        "# --------------------------------------------------------------------\n",
        "with open(loss_txt_path, \"w\") as f:\n",
        "    f.write(f\"loss_weight_{weight_val} = {zero_one_loss:.4f}\\n\")\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" •\", acc_txt_path)\n",
        "print(\" •\", loss_txt_path)"
      ],
      "metadata": {
        "id": "C-5vD5vz9HrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smQnC3aqmU_g"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# display\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.heatmap(df_heatmap,annot=True,fmt='g',cmap='inferno',vmax=100,linewidths=4)\n",
        "plt.title('Incorrect Percentage (%)')\n",
        "plt.savefig(os.path.join(model_evaluation_dir, \"confusion_matrix_{}.jpg\").format(evaluation_target))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ej_IyrLm6xf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "okPBr84WNWW2",
        "7RfaH8aLOO2c",
        "UClQXF5TSzKb",
        "vdAxqNeybKNg",
        "wBuEA_LwbNiP",
        "gCr7VKsKbFxr",
        "5uJczSjQc6Zz",
        "lZ_PCjpU0i3s",
        "YspkQdR7MmEf",
        "9eayGf5X0qZO",
        "ysRr_Xm5VFtN",
        "i4tqGLLeYfoX",
        "uOxMqMfIf3JJ",
        "OQBnBd1on7eo"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}