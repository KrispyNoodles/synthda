{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuCvd5qPVZ3XOP6dh0mhFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NVIDIA/synthda/blob/main/colab/synthda_loop_noCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYb-4YZDjsJt",
        "outputId": "83004cec-ec60-4a29-f2ed-dce4f9e41784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and Setting up dependent repos (Optional if you have your own models you'd like to use instead )"
      ],
      "metadata": {
        "id": "Xw59CK1TdTSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/autosynthda/indiv\n",
        "\n",
        "\n",
        "!git clone https://github.com/NVIDIA/synthda /content/drive/MyDrive/autosynthda/indiv"
      ],
      "metadata": {
        "id": "34MyUItNdhLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/autosynthda/indiv\n",
        "!ls"
      ],
      "metadata": {
        "id": "YzTMKDATdhEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/autosynthda/indiv/components/requirements.txt"
      ],
      "metadata": {
        "id": "bRDVN1kxdhCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install yacs\n",
        "!pip install filterpy\n",
        "!pip install smplx==0.1.28\n",
        "!pip install trimesh==3.9.0\n",
        "!pip install chumpy==0.70\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "id": "_lYk5HrOdg_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hardcoded fix for chumpy for python 3.11\n",
        "\n",
        "import inspect\n",
        "\n",
        "# Monkey patch getargspec for chumpy compatibility with Python ≥3.11\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    from collections import namedtuple\n",
        "\n",
        "    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n",
        "\n",
        "    def getargspec(func):\n",
        "        sig = inspect.signature(func)\n",
        "        args = []\n",
        "        varargs = None\n",
        "        keywords = None\n",
        "        defaults = []\n",
        "\n",
        "        for name, param in sig.parameters.items():\n",
        "            if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n",
        "                args.append(name)\n",
        "                if param.default is not param.empty:\n",
        "                    defaults.append(param.default)\n",
        "            elif param.kind == param.VAR_POSITIONAL:\n",
        "                varargs = name\n",
        "            elif param.kind == param.VAR_KEYWORD:\n",
        "                keywords = name\n",
        "\n",
        "        return ArgSpec(args, varargs, keywords, tuple(defaults) if defaults else None)\n",
        "\n",
        "    inspect.getargspec = getargspec\n"
      ],
      "metadata": {
        "id": "JBNPFS7tdod_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Install Repos"
      ],
      "metadata": {
        "id": "0pFGbWzAdyoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Vegetebird/StridedTransformer-Pose3D.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "!git clone https://github.com/EricGuo5513/text-to-motion.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "!git clone https://github.com/wangsen1312/joints2smpl.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv\n",
        "\n",
        "# Download Blender into the current directory\n",
        "!wget -P . https://download.blender.org/release/Blender3.0/blender-3.0.0-linux-x64.tar.xz\n",
        "\n",
        "# Extract in-place\n",
        "!tar -xf blender-3.0.0-linux-x64.tar.xz -C .\n",
        "\n",
        "#!git clone https://github.com/facebookresearch/SlowFast.git /content/drive/MyDrive/autosynthda/indiv"
      ],
      "metadata": {
        "id": "zYso70lldoat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_vars = \"\"\"\n",
        "STRIDED_TRANSFORMER_PATH=/content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "TEXT_TO_MOTION_PATH=/content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "JOINTS2SMPL_PATH=/content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "SLOWFAST_PATH=/content/drive/MyDrive/autosynthda/indiv/SlowFast\n",
        "BLENDER_BIN=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "BLENDER_ROOT=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64\n",
        "BLENDER_PATH=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/.env\", \"w\") as f:\n",
        "    f.write(env_vars.strip())\n"
      ],
      "metadata": {
        "id": "0Od8Ajp5doXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# switch into the Drive-based repo folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "# make sure the pretrained checkpoint dir exists\n",
        "!mkdir -p checkpoint/pretrained\n",
        "\n",
        "# download the two refine/non-refine checkpoints if missing\n",
        "if not os.path.exists('checkpoint/pretrained/refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1aDLu0SB9JnPYZOOzQsJMV9zEIHg2Uro7 \\\n",
        "            -O checkpoint/pretrained/refine_4365.pth\n",
        "if not os.path.exists('checkpoint/pretrained/no_refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1l63AI9BsNovpfTAbfAkySo9X2MOWgYZH \\\n",
        "            -O checkpoint/pretrained/no_refine_4365.pth\n",
        "\n",
        "# ensure the demo lib checkpoint dir exists\n",
        "!mkdir -p demo/lib/checkpoint\n",
        "\n",
        "# download YOLOv3 weights\n",
        "if not os.path.exists('demo/lib/checkpoint/yolov3.weights'):\n",
        "    !gdown https://drive.google.com/uc?id=1gWZl1VrlLZKBf0Pfkj4hKiFxe8sHP-1C \\\n",
        "            -O demo/lib/checkpoint/yolov3.weights\n",
        "\n",
        "# download HRNet pose model\n",
        "if not os.path.exists('demo/lib/checkpoint/pose_hrnet_w48_384x288.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1CpyZiUIUlEjiql4rILwdBT4666S72Oq4 \\\n",
        "            -O demo/lib/checkpoint/pose_hrnet_w48_384x288.pth\n"
      ],
      "metadata": {
        "id": "6hKq3Q7Udg9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# switch into the Drive-based repo folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# make sure the checkpoints1 dir exists\n",
        "!mkdir -p checkpoints1\n",
        "\n",
        "# download the model checkpoint if missing\n",
        "if not os.path.exists('checkpoints1/checkpoints'):\n",
        "    !gdown https://drive.google.com/uc?id=12liZW5iyvoybXD8eOw4VanTgsMtynCuU \\\n",
        "            -O checkpoints1/checkpoints\n"
      ],
      "metadata": {
        "id": "EIqd-Q4ndg4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Switch into the text-to-motion folder on Drive\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# 3. Ensure the checkpoints1 dir exists\n",
        "!mkdir -p checkpoints1\n",
        "\n",
        "# 4a. Download & unzip the first model\n",
        "if not os.path.exists('checkpoints1/model.zip'):\n",
        "    !gdown --id 12liZW5iyvoybXD8eOw4VanTgsMtynCuU -O checkpoints1/model.zip\n",
        "!unzip -q checkpoints1/model.zip -d checkpoints1\n",
        "\n",
        "# 4b. Download & unzip the second model\n",
        "if not os.path.exists('checkpoints1/model2.zip'):\n",
        "    !gdown --id 1IgrFCnxeg4olBtURUHimzS03ZI0df_6W -O checkpoints1/model2.zip\n",
        "!unzip -q checkpoints1/model2.zip -d checkpoints1\n"
      ],
      "metadata": {
        "id": "6onFvMnBd9kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Each Component for Sanity Check"
      ],
      "metadata": {
        "id": "7J5mxVLsfCFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Note: You should replace the files with the edited versions to make it work in Colab. It can be found in the colab/ folder on the Github"
      ],
      "metadata": {
        "id": "20ADxtPifB2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch into the Drive-based StridedTransformer folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "# run the visualization on your sample video (adjust the path if your video lives elsewhere)\n",
        "!python demo/vis.py --video sample_video.mp4\n"
      ],
      "metadata": {
        "id": "unElOjv_d9hV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch into the Drive-mounted text-to-motion folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# generate motions using your input.txt in that folder\n",
        "!python gen_motion_script.py \\\n",
        "    --name Comp_v6_KLD01 \\\n",
        "    --text_file input.txt \\\n",
        "    --repeat_time 1\n"
      ],
      "metadata": {
        "id": "UXX9LaZRd9eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import smplx\n",
        "importlib.reload(smplx)\n",
        "\n",
        "# switch into the Drive-mounted joints2smpl folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "\n",
        "# list the SMPL model files in your Drive repo\n",
        "!ls -lh /content/drive/MyDrive/autosynthda/indiv/joints2smpl/smpl_models/smpl/\n",
        "\n",
        "# run the fitting script on your test_motion2.npy (make sure the .npy is in this folder or give a full path)\n",
        "!python fit_seq.py --files test_motion2.npy\n"
      ],
      "metadata": {
        "id": "Y4GvZ3B3fHYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source folder in your Drive-mounted joints2smpl repo\n",
        "src = \"/content/drive/MyDrive/autosynthda/indiv/joints2smpl/demo/demo_results/test_motion2\"\n",
        "\n",
        "# Destination folder in your Drive-mounted SynthDA repo\n",
        "dst = \"/content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/test_motion2\"\n",
        "\n",
        "# Ensure the destination parent directory exists\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "\n",
        "# If it already exists, remove the old one\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "# Copy the entire demo_results folder into SynthDA's renders directory\n",
        "shutil.copytree(src, dst)\n",
        "print(\"✅ Folder copied to expected Blender input location.\")\n",
        "\n",
        "# Verify\n",
        "expected_path = dst\n",
        "print(\"Exists:\", os.path.exists(expected_path))\n",
        "print(\"Contents:\", os.listdir(expected_path) if os.path.exists(expected_path) else \"❌ Path does not exist.\")\n",
        "!ls /content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/test_motion2"
      ],
      "metadata": {
        "id": "S-TBepGafHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/content/blender-3.0.0-linux-x64/blender -b -P /content/synthda/components/animation_pose.py -- --name <folder_with_ply_files>\n",
        "\n",
        "# may not work on on Colab immediately, as running blender headlessly on Colab has some challenges. However you will be able to download the .fbx as well to view the animation locally.\n",
        "# For Colab, the suggested alternative is to use pyrender instead, which we use in the loop below!\n",
        "\n",
        "# Step into the folder that contains the script AND the renders/ directory\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/synthda/components\n",
        "\n",
        "# Now launch Blender relative to this directory\n",
        "!/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender \\\n",
        "    -b \\\n",
        "    -P animation_pose.py \\\n",
        "    -- --name test_motion2\n"
      ],
      "metadata": {
        "id": "ItzYyK-Qd9bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SynthDa Full Pipeline without CV model integrated\n",
        "### (CV model training done separately and you would need to feed the acc and loss values into the loop here)"
      ],
      "metadata": {
        "id": "heKiH4H8osSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install rendering & media deps\n",
        "!pip install trimesh pyrender imageio[ffmpeg] pyglet av python-dotenv\n",
        "\n",
        "# Mount Drive (if your code lives there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "bMITkhGmbkfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def generate_sample_metrics(metrics_dir: str, weights: list, acc_values: list, loss_values: list):\n",
        "    \"\"\"\n",
        "    Generate sample accuracy and loss .txt files for different weights.\n",
        "\n",
        "    Args:\n",
        "        metrics_dir (str): The output directory where files will be saved.\n",
        "        weights (list): List of weight floats (e.g., [0.4, 0.5, 0.6]).\n",
        "        acc_values (list): Accuracy values corresponding to weights.\n",
        "        loss_values (list): Loss values corresponding to weights.\n",
        "    \"\"\"\n",
        "    os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "    for w, acc, loss in zip(weights, acc_values, loss_values):\n",
        "        acc_file = os.path.join(metrics_dir, f\"acc_w{w:.2f}.txt\")\n",
        "        loss_file = os.path.join(metrics_dir, f\"loss_w{w:.2f}.txt\")\n",
        "\n",
        "        with open(acc_file, 'w') as af:\n",
        "            af.write(f\"{acc:.4f}\")\n",
        "\n",
        "        with open(loss_file, 'w') as lf:\n",
        "            lf.write(f\"{loss:.4f}\")\n",
        "\n",
        "    print(f\"✅ Sample metrics written to: {metrics_dir}\")\n",
        "\n",
        "# Example: generate for different weights\n",
        "generate_sample_metrics(\n",
        "    metrics_dir='/content/drive/MyDrive/autosynthda/indiv/sample-loss',\n",
        "    weights=[0.40, 0.50, 0.6],\n",
        "    acc_values=[0.60, 0.75, 0.86],\n",
        "    loss_values=[1.3, 0.95, 0.91]\n",
        ")\n"
      ],
      "metadata": {
        "id": "92zMVe7rbkbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key variables for the loop"
      ],
      "metadata": {
        "id": "NkU50g4XbqqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Offscreen EGL for PyRender\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "# Repo roots (adjust as needed)\n",
        "ROOT        = Path('/content/drive/MyDrive/autosynthda/indiv')\n",
        "COMP_ROOT   = ROOT / 'synthda' / 'components'\n",
        "STRIDED     = ROOT / 'StridedTransformer-Pose3D'\n",
        "JOINTS2SMPL = ROOT / 'joints2smpl'\n",
        "\n",
        "# Make sure Python can import your .py files\n",
        "sys.path.extend([\n",
        "    str(ROOT),\n",
        "    str(COMP_ROOT),\n",
        "    str(STRIDED),\n",
        "    str(JOINTS2SMPL),\n",
        "])\n"
      ],
      "metadata": {
        "id": "IOURS8K1bkYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, subprocess, random, math\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/autosynthda/indiv/synthda/components/optimisation')\n",
        "from optimisation.optimisation_utils import (\n",
        "    map_h36m_to_smpl, upsample_pose_data, compute_P_opt\n",
        ")\n",
        "import trimesh, pyrender, imageio\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "4AfhmKOabkWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/autosynthda/indiv/synthda/components\n",
        "# in your /content/synthda/components folder\n",
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "aGvDGQx4bkTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Function Definitions"
      ],
      "metadata": {
        "id": "eIyd6slKbxHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To make it faster, we used pyrender here instead of Blender. But if you are doing it locally, we recc Blender"
      ],
      "metadata": {
        "id": "qwLKnJDdgGxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render_ply_sequence(ply_folder: Path, png_out: Path,\n",
        "                        width=1920, height=1080, fps=27):\n",
        "    png_out.mkdir(parents=True, exist_ok=True)\n",
        "    scene = pyrender.Scene()\n",
        "    cam   = pyrender.PerspectiveCamera(yfov=math.radians(50.0))\n",
        "    light = pyrender.DirectionalLight(color=[1,1,1], intensity=2.0)\n",
        "    scene.add(cam,   pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "    scene.add(light, pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "    r = pyrender.OffscreenRenderer(width, height, point_size=1.0)\n",
        "\n",
        "    ply_files = sorted(ply_folder.glob(\"*.ply\"))\n",
        "    for idx, ply in enumerate(ply_files):\n",
        "        mesh = trimesh.load_mesh(str(ply))\n",
        "        m = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
        "        # clear old mesh\n",
        "        for node in list(scene.mesh_nodes):\n",
        "            scene.remove_node(node)\n",
        "        scene.add(m)\n",
        "        color, _ = r.render(scene)\n",
        "        Image.fromarray(color).save(png_out / f\"frame_{idx:04d}.png\")\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Rendered {idx+1}/{len(ply_files)} frames\")\n",
        "    r.delete()\n"
      ],
      "metadata": {
        "id": "qU-49lMSbxev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def both_real_main(*, weight_A: float, input_dir: str, output_dir: str, num_pairs: int):\n",
        "    random.seed(42)\n",
        "    vids      = list(Path(input_dir).glob(\"*.mp4\"))\n",
        "    pairs     = list(combinations(vids, 2))\n",
        "    selection = random.sample(pairs, min(num_pairs, len(pairs)))\n",
        "\n",
        "    for v1_path, v2_path in selection:\n",
        "        v1, v2    = v1_path.name, v2_path.name\n",
        "        pair_name = v1_path.stem + \"_\" + v2_path.stem\n",
        "        pair_dir  = Path(output_dir) / pair_name\n",
        "        pair_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 1) Copy & StridedTransformer visualisation\n",
        "        for vn in (v1, v2):\n",
        "            # keep a copy\n",
        "            shutil.copy(Path(input_dir)/vn, pair_dir/vn)\n",
        "            # stage for vis\n",
        "            dst = STRIDED / \"demo\" / \"video\" / vn\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(Path(input_dir)/vn, dst)\n",
        "            subprocess.run(\n",
        "                [\"python\",\"demo/vis.py\",\"--video\", vn],\n",
        "                cwd=STRIDED, check=True)\n",
        "\n",
        "        # 2) Map NPZ → SMPL .npy\n",
        "        npz1 = STRIDED/\"demo\"/\"output\"/v1_path.stem/\"output_3D\"/\"output_keypoints_3d.npz\"\n",
        "        npz2 = STRIDED/\"demo\"/\"output\"/v2_path.stem/\"output_3D\"/\"output_keypoints_3d.npz\"\n",
        "        out1 = pair_dir/\"output_keypoints_3d_real1.npy\"\n",
        "        out2 = pair_dir/\"output_keypoints_3d_real2.npy\"\n",
        "        np.save(out1, map_h36m_to_smpl(str(npz1)))\n",
        "        np.save(out2, map_h36m_to_smpl(str(npz2)))\n",
        "\n",
        "        # 3) Align & upsample\n",
        "        p1, p2 = np.load(out1), np.load(out2)\n",
        "        if p1.shape[0] < p2.shape[0]:\n",
        "            np.save(out1, upsample_pose_data(str(out1), p2.shape[0]))\n",
        "        else:\n",
        "            np.save(out2, upsample_pose_data(str(out2), p1.shape[0]))\n",
        "\n",
        "        # 4) Generate weighted mixtures\n",
        "        var_dir = pair_dir/\"all_variations\"\n",
        "        var_dir.mkdir(exist_ok=True, parents=True)\n",
        "        for wA in [round(x,1) for x in np.linspace(0.1,0.9,9)]:\n",
        "            wB   = round(1-wA,1)\n",
        "            Popt = compute_P_opt(str(out1), str(out2), alpha=0.5, w_A=wA, w_B=wB)\n",
        "            np.save(var_dir/f\"euclidean_wA{wA}_wB{wB}.npy\", Popt)\n",
        "\n",
        "        # 5) Pick target variation\n",
        "        target = var_dir/f\"euclidean_wA{weight_A}_wB{round(1-weight_A,1)}.npy\"\n",
        "        if not target.exists():\n",
        "            raise FileNotFoundError(f\"Missing variation: {target}\")\n",
        "\n",
        "        # 6) joints2smpl → .ply\n",
        "        dest_npy = JOINTS2SMPL/\"demo\"/\"demo_data\"/target.name\n",
        "        dest_npy.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy(target, dest_npy)\n",
        "        subprocess.run(\n",
        "            [\"python\",\"fit_seq.py\",\"--files\", dest_npy.name, \"--num_smplify_iters\",\"1\"],\n",
        "            cwd=JOINTS2SMPL, check=True)\n",
        "        ply_src = JOINTS2SMPL/\"demo\"/\"demo_results\"/dest_npy.stem\n",
        "\n",
        "        # 7) Render PLY → PNG\n",
        "        png_folder = COMP_ROOT/\"renders\"/f\"{pair_name}_{dest_npy.stem}\"\n",
        "        render_ply_sequence(ply_src, png_folder)\n",
        "\n",
        "        # 8) Stitch PNG → MP4\n",
        "        out_mp4 = Path(output_dir)/f\"videos_real2_{weight_A}\"/f\"{pair_name}.mp4\"\n",
        "        out_mp4.parent.mkdir(parents=True, exist_ok=True)\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\",\"-y\",\n",
        "            \"-framerate\",\"27\",\n",
        "            \"-i\", str(png_folder/\"frame_%04d.png\"),\n",
        "            \"-c:v\",\"libx264\",\"-pix_fmt\",\"yuv420p\",\n",
        "            str(out_mp4)\n",
        "        ], check=True)\n",
        "\n",
        "        print(\"✅ Generated:\", out_mp4)\n"
      ],
      "metadata": {
        "id": "gXjCb_mmbxwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate starting videos for first comparisons"
      ],
      "metadata": {
        "id": "BAP3fwgEb5pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "weights = [0.4, 0.5, 0.6]\n",
        "input_dir  = str(COMP_ROOT/\"dataset\"/\"specific_data\")\n",
        "output_dir = str(COMP_ROOT/\"dataset\"/\"data_manipulation\")\n",
        "\n",
        "for w in weights:\n",
        "    print(f\"\\n Now running both_real_main with weight {w}\")\n",
        "    both_real_main(\n",
        "        weight_A=w,\n",
        "        input_dir=input_dir,\n",
        "        output_dir=output_dir,\n",
        "        num_pairs=1\n",
        "    )\n"
      ],
      "metadata": {
        "id": "mfZLdkrWbxuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── TODO: Run your model on the generated videos to compute loss & FALL accuracy ───\n",
        "# User to provide: loss_w{w}.txt and acc_w{w}.txt file paths for each w in [0.4,0.5,0.6]\n",
        "# this would be the baseline accuracy that we would compare against\n"
      ],
      "metadata": {
        "id": "EjabGM-dbxrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SynthDa Loop"
      ],
      "metadata": {
        "id": "tGYdoRr3cACZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ─── 1) Ask for the directory containing your loss files ──────────────────────\n",
        "loss_dir = input(\n",
        "    \"Enter the path to the folder containing your loss_w{weight}.txt files:\\n\"\n",
        ")\n",
        "\n",
        "def evaluate_loss(weight: float) -> float:\n",
        "    \"\"\"\n",
        "    Reads the model loss for the given weight from a text file.\n",
        "    Supports 3 decimal places (e.g., loss_w0.643.txt).\n",
        "    \"\"\"\n",
        "    fname = f\"loss_w{weight:.3f}.txt\"\n",
        "    fpath = os.path.join(loss_dir, fname)\n",
        "    if not os.path.isfile(fpath):\n",
        "        fpath = input(f\"File {fname} not found. Enter full path to loss file for weight {weight:.3f}:\\n\")\n",
        "    return float(open(fpath).read().strip())\n",
        "\n",
        "\n",
        "def calculate_new_weight(prev_w: float, prev_loss: float, step: float):\n",
        "    \"\"\"\n",
        "    Uses finite differences: compares loss at prev_w±step,\n",
        "    and returns the weight with the lowest loss among [low, prev, high],\n",
        "    allowing full floating-point precision.\n",
        "    \"\"\"\n",
        "    low  = max(prev_w - step, 0.0)\n",
        "    high = min(prev_w + step, 1.0)\n",
        "\n",
        "    # Keep full precision in filenames (up to 3 dp recommended, but to make it faster you can reduce to 1dp)\n",
        "    loss_low  = evaluate_loss(round(low, 3))\n",
        "    loss_high = evaluate_loss(round(high, 3))\n",
        "    candidates = [\n",
        "        (round(low,  3), loss_low),\n",
        "        (round(prev_w, 3), prev_loss),\n",
        "        (round(high, 3), loss_high)\n",
        "    ]\n",
        "    best_w, best_loss = min(candidates, key=lambda x: x[1])\n",
        "    return best_w, best_loss\n"
      ],
      "metadata": {
        "id": "igF_Hjx7bkRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 2) Initialize ───────────────────────────────────────────────────────────\n",
        "prev_w    = float(input(\"Start weight (e.g. 0.5):\\n\") or 0.5)\n",
        "prev_loss = evaluate_loss(prev_w)\n",
        "step     = 0.1\n",
        "tol      = 1e-3    # convergence threshold\n",
        "max_iter = 20\n",
        "\n",
        "print(f\"\\nStarting at weight {prev_w:.3f} with loss {prev_loss:.3f}\\n\")\n",
        "\n",
        "# ─── 3) Iterative loop ───────────────────────────────────────────────────────\n",
        "for iteration in range(1, max_iter + 1):\n",
        "    # Propose a new weight\n",
        "    next_w, next_loss = calculate_new_weight(prev_w, prev_loss, step)\n",
        "    print(f\"Iteration {iteration}: suggested weight {next_w:.3f} (prev loss {prev_loss:.3f} → new loss {next_loss:.3f})\")\n",
        "\n",
        "    # 3a) Run your real2real pipeline on next_w\n",
        "    both_real_main(\n",
        "        weight_A=next_w,\n",
        "        input_dir=input_dir,     # reuse same input_dir defined earlier\n",
        "        output_dir=output_dir,   # reuse same output_dir defined earlier\n",
        "        num_pairs=1\n",
        "    )\n",
        "\n",
        "    # 3b) Prompt user to run SlowFast on the newly generated videos\n",
        "    print(f\"\\nNow run SlowFast on videos_real2_{next_w:.3f} and place loss_w{next_w:.2f}.txt in:\\n  {loss_dir}\")\n",
        "    input(\"Press Enter when the new loss file is ready…\")\n",
        "\n",
        "    # 3c) Re-evaluate loss\n",
        "    new_loss = evaluate_loss(next_w)\n",
        "    print(f\"→ New loss at weight {next_w:.3f}: {new_loss:.3f}\\n\")\n",
        "\n",
        "    # 3d) Check for convergence\n",
        "    if abs(new_loss - prev_loss) < tol:\n",
        "        print(f\"Converged (Δloss {abs(new_loss - prev_loss):.4f} < {tol})\")\n",
        "        break\n",
        "\n",
        "    # 3e) Prepare for next iteration\n",
        "    prev_w, prev_loss = next_w, new_loss\n",
        "    step = step / 2  # Optionally shrink step each round\n",
        "\n",
        "else:\n",
        "    print(f\"Reached max iterations ({max_iter}) without full convergence.\")"
      ],
      "metadata": {
        "id": "tb6Kf-hMbkOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}