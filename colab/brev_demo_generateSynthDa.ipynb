{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0pFGbWzAdyoK"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPDqbhi9kMGxAi5hsu1Z0UE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NVIDIA/synthda/blob/main/colab/brev_demo_generateSynthDa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYb-4YZDjsJt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and Setting up SynthDa repo (Optional if you have your own models you'd like to use instead )"
      ],
      "metadata": {
        "id": "Xw59CK1TdTSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/autosynthda/indiv\n",
        "\n",
        "\n",
        "!git clone https://github.com/NVIDIA/synthda /content/drive/MyDrive/autosynthda/indiv"
      ],
      "metadata": {
        "id": "34MyUItNdhLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements for each new runtime"
      ],
      "metadata": {
        "id": "cUG_w9f0SqeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/autosynthda/indiv\n",
        "!ls"
      ],
      "metadata": {
        "id": "YzTMKDATdhEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/autosynthda/indiv/synthda/components/requirements.txt"
      ],
      "metadata": {
        "id": "bRDVN1kxdhCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install yacs\n",
        "!pip install filterpy\n",
        "!pip install smplx==0.1.28\n",
        "!pip install trimesh==3.9.0\n",
        "!pip install chumpy==0.70\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "id": "_lYk5HrOdg_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hardcoded fix for chumpy for python 3.11\n",
        "\n",
        "import inspect\n",
        "\n",
        "# Monkey patch getargspec for chumpy compatibility with Python â‰¥3.11\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    from collections import namedtuple\n",
        "\n",
        "    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n",
        "\n",
        "    def getargspec(func):\n",
        "        sig = inspect.signature(func)\n",
        "        args = []\n",
        "        varargs = None\n",
        "        keywords = None\n",
        "        defaults = []\n",
        "\n",
        "        for name, param in sig.parameters.items():\n",
        "            if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n",
        "                args.append(name)\n",
        "                if param.default is not param.empty:\n",
        "                    defaults.append(param.default)\n",
        "            elif param.kind == param.VAR_POSITIONAL:\n",
        "                varargs = name\n",
        "            elif param.kind == param.VAR_KEYWORD:\n",
        "                keywords = name\n",
        "\n",
        "        return ArgSpec(args, varargs, keywords, tuple(defaults) if defaults else None)\n",
        "\n",
        "    inspect.getargspec = getargspec\n"
      ],
      "metadata": {
        "id": "JBNPFS7tdod_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Install Repos [If done previously, then can skip this]"
      ],
      "metadata": {
        "id": "0pFGbWzAdyoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Vegetebird/StridedTransformer-Pose3D.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "!git clone https://github.com/EricGuo5513/text-to-motion.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "!git clone https://github.com/wangsen1312/joints2smpl.git \\\n",
        "             /content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv\n",
        "\n",
        "# Download Blender into the current directory\n",
        "!wget -P . https://download.blender.org/release/Blender3.0/blender-3.0.0-linux-x64.tar.xz\n",
        "\n",
        "# Extract in-place\n",
        "!tar -xf blender-3.0.0-linux-x64.tar.xz -C .\n",
        "\n",
        "#!git clone https://github.com/facebookresearch/SlowFast.git /content/drive/MyDrive/autosynthda/indiv"
      ],
      "metadata": {
        "id": "zYso70lldoat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# switch into the Drive-based repo folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "# make sure the pretrained checkpoint dir exists\n",
        "!mkdir -p checkpoint/pretrained\n",
        "\n",
        "# download the two refine/non-refine checkpoints if missing\n",
        "if not os.path.exists('checkpoint/pretrained/refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1aDLu0SB9JnPYZOOzQsJMV9zEIHg2Uro7 \\\n",
        "            -O checkpoint/pretrained/refine_4365.pth\n",
        "if not os.path.exists('checkpoint/pretrained/no_refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1l63AI9BsNovpfTAbfAkySo9X2MOWgYZH \\\n",
        "            -O checkpoint/pretrained/no_refine_4365.pth\n",
        "\n",
        "# ensure the demo lib checkpoint dir exists\n",
        "!mkdir -p demo/lib/checkpoint\n",
        "\n",
        "# download YOLOv3 weights\n",
        "if not os.path.exists('demo/lib/checkpoint/yolov3.weights'):\n",
        "    !gdown https://drive.google.com/uc?id=1gWZl1VrlLZKBf0Pfkj4hKiFxe8sHP-1C \\\n",
        "            -O demo/lib/checkpoint/yolov3.weights\n",
        "\n",
        "# download HRNet pose model\n",
        "if not os.path.exists('demo/lib/checkpoint/pose_hrnet_w48_384x288.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1CpyZiUIUlEjiql4rILwdBT4666S72Oq4 \\\n",
        "            -O demo/lib/checkpoint/pose_hrnet_w48_384x288.pth\n"
      ],
      "metadata": {
        "id": "6hKq3Q7Udg9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# switch into the Drive-based repo folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# make sure the checkpoints1 dir exists\n",
        "!mkdir -p checkpoints1\n",
        "\n",
        "# download the model checkpoint if missing\n",
        "if not os.path.exists('checkpoints1/checkpoints'):\n",
        "    !gdown https://drive.google.com/uc?id=12liZW5iyvoybXD8eOw4VanTgsMtynCuU \\\n",
        "            -O checkpoints1/checkpoints\n"
      ],
      "metadata": {
        "id": "EIqd-Q4ndg4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Switch into the text-to-motion folder on Drive\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# 3. Ensure the checkpoints1 dir exists\n",
        "!mkdir -p checkpoints1\n",
        "\n",
        "# 4a. Download & unzip the first model\n",
        "if not os.path.exists('checkpoints1/model.zip'):\n",
        "    !gdown --id 12liZW5iyvoybXD8eOw4VanTgsMtynCuU -O checkpoints1/model.zip\n",
        "!unzip -q checkpoints1/model.zip -d checkpoints1\n",
        "\n",
        "# 4b. Download & unzip the second model\n",
        "if not os.path.exists('checkpoints1/model2.zip'):\n",
        "    !gdown --id 1IgrFCnxeg4olBtURUHimzS03ZI0df_6W -O checkpoints1/model2.zip\n",
        "!unzip -q checkpoints1/model2.zip -d checkpoints1\n"
      ],
      "metadata": {
        "id": "6onFvMnBd9kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Each Component for Sanity Check"
      ],
      "metadata": {
        "id": "7J5mxVLsfCFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install iopath fvcore pytorchvideo tensorboard setuptools torchinfo opencv-python seaborn numpy Pillow scikit-learn\n"
      ],
      "metadata": {
        "id": "5dnJ-9nOXked"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the version of torch and whether cuda is used\n",
        "import torch\n",
        "print(\"Torch file:\", torch.__file__)\n",
        "print(\"CUDA attr exists:\", hasattr(torch, \"cuda\"))\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Device count:\", torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "A0plBGPtXCc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_vars = \"\"\"\n",
        "STRIDED_TRANSFORMER_PATH=/content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "TEXT_TO_MOTION_PATH=/content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "JOINTS2SMPL_PATH=/content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "SLOWFAST_PATH=/content/drive/MyDrive/autosynthda/indiv/SlowFast\n",
        "BLENDER_BIN=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "BLENDER_ROOT=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64\n",
        "BLENDER_PATH=/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "\"\"\"\n",
        "with open(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/.env\", \"w\") as f:\n",
        "    f.write(env_vars.strip())\n"
      ],
      "metadata": {
        "id": "0Od8Ajp5doXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Note: You should replace the files with the edited versions to make it work in Colab. It can be found in the colab/ folder on the Github"
      ],
      "metadata": {
        "id": "20ADxtPifB2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch into the Drive-based StridedTransformer folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "# run the visualization on your sample video (adjust the path if your video lives elsewhere)\n",
        "!python demo/vis.py --video sample_video.mp4\n"
      ],
      "metadata": {
        "id": "unElOjv_d9hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch into the Drive-mounted text-to-motion folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/text-to-motion\n",
        "\n",
        "# generate motions using your input.txt in that folder\n",
        "!python gen_motion_script.py \\\n",
        "    --name Comp_v6_KLD01 \\\n",
        "    --text_file input.txt \\\n",
        "    --repeat_time 1\n"
      ],
      "metadata": {
        "id": "UXX9LaZRd9eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import smplx\n",
        "importlib.reload(smplx)\n",
        "\n",
        "# switch into the Drive-mounted joints2smpl folder\n",
        "%cd /content/drive/MyDrive/autosynthda/indiv/joints2smpl\n",
        "\n",
        "# list the SMPL model files in your Drive repo\n",
        "!ls -lh /content/drive/MyDrive/autosynthda/indiv/joints2smpl/smpl_models/smpl/\n",
        "\n",
        "# run the fitting script on your test_motion2.npy (make sure the .npy is in this folder or give a full path)\n",
        "!python fit_seq.py --files test_motion2.npy\n"
      ],
      "metadata": {
        "id": "Y4GvZ3B3fHYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source folder in your Drive-mounted joints2smpl repo\n",
        "src = \"/content/drive/MyDrive/autosynthda/indiv/joints2smpl/demo/demo_results/test_motion2\"\n",
        "\n",
        "# Destination folder in your Drive-mounted SynthDA repo\n",
        "dst = \"/content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/test_motion2\"\n",
        "\n",
        "# Ensure the destination parent directory exists\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "\n",
        "# If it already exists, remove the old one\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "# Copy the entire demo_results folder into SynthDA's renders directory\n",
        "shutil.copytree(src, dst)\n",
        "print(\"âœ… Folder copied to expected Blender input location.\")\n",
        "\n",
        "# Verify\n",
        "expected_path = dst\n",
        "print(\"Exists:\", os.path.exists(expected_path))\n",
        "print(\"Contents:\", os.listdir(expected_path) if os.path.exists(expected_path) else \"Path does not exist.\")\n",
        "!ls /content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/test_motion2"
      ],
      "metadata": {
        "id": "S-TBepGafHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/content/blender-3.0.0-linux-x64/blender -b -P /content/synthda/components/animation_pose.py -- --name <folder_with_ply_files>\n",
        "\n",
        "# may not work on on Colab immediately, as running blender headlessly on Colab has some challenges. However you will be able to download the .fbx as well to view the animation locally.\n",
        "# For Colab, the suggested alternative is to use pyrender instead, which we use in the demo function created for BREV below!\n",
        "\n",
        "# Step into the folder that contains the script AND the renders/ directory\n",
        "# %cd /content/drive/MyDrive/autosynthda/indiv/synthda/components\n",
        "\n",
        "# Now launch Blender relative to this directory\n",
        "#!/content/drive/MyDrive/autosynthda/indiv/blender-3.0.0-linux-x64/blender \\\n",
        "#    -b \\\n",
        "#    -P animation_pose.py \\\n",
        "#    -- --name test_motion2\n"
      ],
      "metadata": {
        "id": "ItzYyK-Qd9bO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Your Own Demo Data [Trying out Real-Mix from SynthDa]"
      ],
      "metadata": {
        "id": "ipIi5J3oUpEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh pyrender imageio[ffmpeg] pyglet\n",
        "!pip install av imageio[ffmpeg] trimesh pyrender pyglet\n",
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "id": "QvnYoc2xVz0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components\")\n",
        "\n",
        "from optimisation.optimisation_utils import map_h36m_to_smpl, upsample_pose_data, compute_P_opt\n",
        "\n",
        "# real_path_npz_1 refers to the First Generated Tracked-Motion Data\n",
        "# real_path_npz_2 refers to the Second Generated Tracked-Motion Data\n",
        "\n",
        "# used when one is real and another is real too\n",
        "def main_real_real(real_path_npz_1, real_path_npz_2, folder_path):\n",
        "\n",
        "    # folder_path_variations = folder_path + \"all_variations/\"\n",
        "    # print(folder_path_variations)\n",
        "    folder_path_variations = Path(folder_path) / \"all_variations\"\n",
        "    folder_path_variations.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # converting npz to npy\n",
        "    real_data_1_path = folder_path + '/output_keypoints_3d_real1.npy'\n",
        "    real_data_2_path = folder_path + '/output_keypoints_3d_real2.npy'\n",
        "\n",
        "    print(real_data_1_path)\n",
        "    print(real_data_2_path)\n",
        "\n",
        "    # mapping the npz to npy file with 22 joints\n",
        "    real_data_1 = map_h36m_to_smpl(real_path_npz_1)\n",
        "    np.save(real_data_1_path, real_data_1)\n",
        "\n",
        "    # mapping the npz to npy file with 22 joints\n",
        "    real_data_2 = map_h36m_to_smpl(real_path_npz_2)\n",
        "    np.save(real_data_2_path, real_data_2)\n",
        "\n",
        "    # checking which frames are shorter, to match the frames to be the same\n",
        "    # Load the real and synthetic pose data from .npy files\n",
        "    P_r = np.load(real_data_1_path)  # Shape: (J, 3)\n",
        "    P_s = np.load(real_data_2_path)  # Shape: (J, 3)\n",
        "\n",
        "    # Determine the smaller array\n",
        "    if P_r.shape[0] > P_s.shape[0]:\n",
        "        smaller_array = real_data_2_path\n",
        "        bigger_array = real_data_1_path\n",
        "        max_frames = P_r.shape[0]\n",
        "    else:\n",
        "        smaller_array = real_data_1_path\n",
        "        bigger_array = real_data_2_path\n",
        "        max_frames = P_s.shape[0]\n",
        "\n",
        "    # upsampling the smaller array to the number of frames\n",
        "    pose_data_upsampled = upsample_pose_data(smaller_array, target_frames=max_frames)\n",
        "\n",
        "    extended_new_path = smaller_array.replace(\".npy\", \"_extended.npy\")\n",
        "\n",
        "    # saving the new array\n",
        "    np.save(extended_new_path, pose_data_upsampled)\n",
        "    print(pose_data_upsampled.shape)\n",
        "\n",
        "    # generating the all variations of optimisation files\n",
        "    weight_pairs = [\n",
        "        (0.1, 0.9),\n",
        "        (0.2, 0.8),\n",
        "        (0.3, 0.7),\n",
        "        (0.4, 0.6),\n",
        "        (0.5, 0.5),\n",
        "        (0.6, 0.4),\n",
        "        (0.7, 0.3),\n",
        "        (0.8, 0.2),\n",
        "        (0.9, 0.1)\n",
        "    ]\n",
        "\n",
        "    if bigger_array == real_data_1_path:\n",
        "        for w_A, w_B in weight_pairs:\n",
        "            P_opt = compute_P_opt(real_data_1_path, extended_new_path, alpha=0.5, w_A=w_A, w_B=w_B)\n",
        "            np.save(f\"{folder_path_variations}/_euclidean_distances_wA{w_A}_wB{w_B}.npy\", P_opt)\n",
        "\n",
        "    # in a situation where the real_path is shorter than the synthetic path\n",
        "    if smaller_array == real_data_1_path:\n",
        "        for w_A, w_B in weight_pairs:\n",
        "            P_opt = compute_P_opt(real_data_2_path, extended_new_path, alpha=0.5, w_A=w_A, w_B=w_B)\n",
        "            np.save(f\"{folder_path_variations}/_euclidean_distances_wA{w_A}_wB{w_B}.npy\", P_opt)\n"
      ],
      "metadata": {
        "id": "Q-7XP5RnUx2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# try EGL first:\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "# if that still complains you can try:\n",
        "# os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "\n",
        "from pathlib import Path\n",
        "import math\n",
        "import trimesh\n",
        "import pyrender\n",
        "from PIL import Image\n",
        "\n",
        "# ---- adjust these paths ----\n",
        "ply_dir = Path(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/test_video_3_test_video_1_euclidean_distances_wA0.5_wB0.5\")\n",
        "out_dir = Path(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/renders/testpng_video3_gen_frames\")\n",
        "# -----------------------------\n",
        "\n",
        "out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# build a single scene/camera/light\n",
        "scene = pyrender.Scene()\n",
        "camera = pyrender.PerspectiveCamera(yfov=math.radians(50.0))\n",
        "light  = pyrender.DirectionalLight(color=[1.0,1.0,1.0], intensity=2.0)\n",
        "scene.add(camera, pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "scene.add(light,  pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "\n",
        "# now the offscreen renderer\n",
        "r = pyrender.OffscreenRenderer(viewport_width=1920,\n",
        "                               viewport_height=1080,\n",
        "                               point_size=1.0)\n",
        "\n",
        "files = sorted(ply_dir.glob(\"*.ply\"))\n",
        "for i, ply in enumerate(files):\n",
        "    mesh = trimesh.load_mesh(ply)\n",
        "    # if your models need upright correction:\n",
        "    # mesh.apply_transform(trimesh.transformations.rotation_matrix(\n",
        "    #     math.radians(90), [1,0,0]))\n",
        "    m = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
        "\n",
        "    # swap out old mesh\n",
        "    for node in list(scene.mesh_nodes):\n",
        "        scene.remove_node(node)\n",
        "    scene.add(m)\n",
        "\n",
        "    color, _ = r.render(scene)\n",
        "    Image.fromarray(color).save(out_dir / f\"frame_{i:04d}.png\")\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Rendered {i+1}/{len(files)} frames\")\n",
        "\n",
        "r.delete()\n",
        "print(\"âœ… All frames saved in\", out_dir)\n"
      ],
      "metadata": {
        "id": "hiNriM4CU8s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "import math\n",
        "import trimesh\n",
        "import pyrender\n",
        "from PIL import Image\n",
        "\n",
        "# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "env = dotenv_values(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/.env\")\n",
        "STRIDED     = Path(env[\"STRIDED_TRANSFORMER_PATH\"])\n",
        "TEXT2M      = Path(env[\"TEXT_TO_MOTION_PATH\"])\n",
        "JOINTS2SMPL = Path(env[\"JOINTS2SMPL_PATH\"])\n",
        "COMP_ROOT   = Path(\"/content/drive/MyDrive/autosynthda/indiv/synthda/components\")\n",
        "ANIM_REND   = COMP_ROOT/\"renders\"  # base for saving plyâ†’png sequences\n",
        "\n",
        "# optimisation utils\n",
        "from optimisation.optimisation_utils import map_h36m_to_smpl, upsample_pose_data, compute_P_opt\n",
        "\n",
        "# â”€â”€â”€ Helper: headless PLYâ†’PNG via pyrender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def render_ply_sequence(ply_folder: Path, png_out: Path,\n",
        "                        width=1920, height=1080, fps=27):\n",
        "    # set EGL for headless\n",
        "    os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "\n",
        "    png_out.mkdir(parents=True, exist_ok=True)\n",
        "    scene = pyrender.Scene()\n",
        "    cam = pyrender.PerspectiveCamera(yfov=math.radians(50.0))\n",
        "    light = pyrender.DirectionalLight(color=[1.0,1.0,1.0], intensity=2.0)\n",
        "    scene.add(cam, pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "    scene.add(light,  pose=trimesh.transformations.translation_matrix([0,0,2]))\n",
        "    r = pyrender.OffscreenRenderer(viewport_width=width,\n",
        "                                   viewport_height=height,\n",
        "                                   point_size=1.0)\n",
        "\n",
        "    ply_files = sorted(ply_folder.glob(\"*.ply\"))\n",
        "    for idx, ply in enumerate(ply_files):\n",
        "        mesh = trimesh.load_mesh(str(ply))\n",
        "        # if you need the 90Â° upright correction, uncomment:\n",
        "        # mesh.apply_transform(trimesh.transformations.rotation_matrix(\n",
        "        #     math.radians(90), [1,0,0]))\n",
        "        m = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
        "        # clear old mesh nodes\n",
        "        for node in list(scene.mesh_nodes):\n",
        "            scene.remove_node(node)\n",
        "        scene.add(m)\n",
        "        color, _ = r.render(scene)\n",
        "        Image.fromarray(color).save(png_out/f\"frame_{idx:04d}.png\")\n",
        "    r.delete()\n",
        "\n",
        "# â”€â”€â”€ Main pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def both_real_main(*, weight_A, input_dir, output_dir, num_pairs):\n",
        "    random.seed(42)\n",
        "    video_dir      = Path(input_dir)\n",
        "    out_root       = Path(output_dir)\n",
        "    video_gen_root = video_dir.parent/f\"videos_generated_real2_{weight_A}\"\n",
        "    video_gen_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    vids = list(video_dir.glob(\"*.mp4\"))\n",
        "    pairs = list(combinations(vids, 2))\n",
        "    selection = random.sample(pairs, min(num_pairs, len(pairs)))\n",
        "\n",
        "    for v1_path, v2_path in selection:\n",
        "        v1, v2      = v1_path.name, v2_path.name\n",
        "        pair_name   = v1_path.stem + \"_\" + v2_path.stem\n",
        "        pair_folder = out_root / pair_name\n",
        "        pair_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # copy originals\n",
        "        shutil.copy(v1_path, pair_folder/v1)\n",
        "        shutil.copy(v2_path, pair_folder/v2)\n",
        "\n",
        "        # 1) StridedTransformer\n",
        "        for vn in (v1, v2):\n",
        "            tgt = STRIDED/\"demo\"/\"video\"/vn\n",
        "            tgt.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(pair_folder/vn, tgt)\n",
        "            subprocess.run(\n",
        "                [\"python\",\"demo/vis.py\",\"--video\",vn],\n",
        "                cwd=STRIDED, check=True\n",
        "            )\n",
        "\n",
        "        # 2) map NPZ â†’ SMPL .npy\n",
        "        npz1 = STRIDED/\"demo\"/\"output\"/v1_path.stem/\"output_3D\"/\"output_keypoints_3d.npz\"\n",
        "        npz2 = STRIDED/\"demo\"/\"output\"/v2_path.stem/\"output_3D\"/\"output_keypoints_3d.npz\"\n",
        "        out1 = pair_folder/\"output_keypoints_3d_real1.npy\"\n",
        "        out2 = pair_folder/\"output_keypoints_3d_real2.npy\"\n",
        "        p1 = map_h36m_to_smpl(str(npz1)); np.save(out1, p1)\n",
        "        p2 = map_h36m_to_smpl(str(npz2)); np.save(out2, p2)\n",
        "\n",
        "        # align lengths\n",
        "        if p1.shape[0] < p2.shape[0]:\n",
        "            p1 = upsample_pose_data(str(out1), p2.shape[0]); np.save(out1,p1)\n",
        "        else:\n",
        "            p2 = upsample_pose_data(str(out2), p1.shape[0]); np.save(out2,p2)\n",
        "\n",
        "        # 3) generate mixtures\n",
        "        var_folder = pair_folder/\"all_variations\"\n",
        "        var_folder.mkdir(exist_ok=True, parents=True)\n",
        "        for wA in [round(x,1) for x in np.linspace(0.1,0.9,9)]:\n",
        "            wB = round(1-wA,1)\n",
        "            Popt = compute_P_opt(str(out1), str(out2),\n",
        "                                  alpha=0.5, w_A=wA, w_B=wB)\n",
        "            np.save(var_folder/f\"euclidean_distances_wA{wA}_wB{wB}.npy\", Popt)\n",
        "\n",
        "        # pick the requested mix\n",
        "        target_npy = var_folder/f\"euclidean_distances_wA{weight_A}_wB{round(1-weight_A,1)}.npy\"\n",
        "        if not target_npy.exists():\n",
        "            raise FileNotFoundError(f\"Missing variation: {target_npy}\")\n",
        "\n",
        "        # 4) joints2smpl â†’ .ply frames\n",
        "        dest_npy = JOINTS2SMPL/\"demo\"/\"demo_data\"/(pair_name+\"_\"+target_npy.name)\n",
        "        dest_npy.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy(target_npy, dest_npy)\n",
        "        subprocess.run(\n",
        "            [\"python\",\"fit_seq.py\",\"--files\",dest_npy.name,\"--num_smplify_iters\",\"1\"],\n",
        "            cwd=JOINTS2SMPL, check=True\n",
        "        )\n",
        "\n",
        "        # find the .ply output folder\n",
        "        ply_src = JOINTS2SMPL/\"demo\"/\"demo_results\"/dest_npy.stem\n",
        "        if not ply_src.exists():\n",
        "            raise FileNotFoundError(f\"joints2smpl output not found: {ply_src}\")\n",
        "\n",
        "        # 5) render PLY â†’ PNG\n",
        "        png_folder = ANIM_REND/f\"{pair_name}_{target_npy.stem}\"\n",
        "        render_ply_sequence(ply_src, png_folder)\n",
        "\n",
        "        # 6) ffmpeg assemble\n",
        "        out_mp4 = video_gen_root/f\"{pair_name}.mp4\"\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\",\"-y\",\n",
        "            \"-framerate\", str(27),\n",
        "            \"-i\", str(png_folder/\"frame_%04d.png\"),\n",
        "            \"-c:v\",\"libx264\",\"-pix_fmt\",\"yuv420p\",\n",
        "            str(out_mp4)\n",
        "        ], check=True)\n",
        "\n",
        "        print(\"âœ… Generated:\", out_mp4)\n"
      ],
      "metadata": {
        "id": "EpuhtZenU-93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_real_main(\n",
        "  weight_A=0.2,\n",
        "  input_dir=\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/dataset/specific_data\",\n",
        "  output_dir=\"/content/drive/MyDrive/autosynthda/indiv/synthda/components/dataset/data_manipulation\",\n",
        "  num_pairs=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "tFEmo9DJVAlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u13y73EYWC2A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}