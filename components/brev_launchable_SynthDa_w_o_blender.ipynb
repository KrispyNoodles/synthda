{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYb-4YZDjsJt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and Setting up SynthDa repo (Optional if you have your own models you'd like to use instead )"
      ],
      "metadata": {
        "id": "Xw59CK1TdTSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a local directory for autosynthda project\n",
        "mkdir -p ~/autosynthda/indiv\n",
        "\n",
        "# Clone the synthda repo into that directory\n",
        "git clone https://github.com/NVIDIA/synthda ~/autosynthda/indiv\n"
      ],
      "metadata": {
        "id": "34MyUItNdhLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements for each new runtime"
      ],
      "metadata": {
        "id": "cUG_w9f0SqeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/autosynthda/indiv\n",
        "!ls"
      ],
      "metadata": {
        "id": "YzTMKDATdhEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r ~/autosynthda/indiv/synthda/components/requirements.txt\n"
      ],
      "metadata": {
        "id": "bRDVN1kxdhCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \\\n",
        "    numpy==1.23.5 \\\n",
        "    yacs \\\n",
        "    filterpy \\\n",
        "    smplx==0.1.28 \\\n",
        "    trimesh==3.9.0 \\\n",
        "    chumpy==0.70 \\\n",
        "    python-dotenv\n"
      ],
      "metadata": {
        "id": "_lYk5HrOdg_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hardcoded fix for chumpy for python 3.11\n",
        "\n",
        "import inspect\n",
        "\n",
        "# Monkey patch getargspec for chumpy compatibility with Python â‰¥3.11\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    from collections import namedtuple\n",
        "\n",
        "    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n",
        "\n",
        "    def getargspec(func):\n",
        "        sig = inspect.signature(func)\n",
        "        args = []\n",
        "        varargs = None\n",
        "        keywords = None\n",
        "        defaults = []\n",
        "\n",
        "        for name, param in sig.parameters.items():\n",
        "            if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n",
        "                args.append(name)\n",
        "                if param.default is not param.empty:\n",
        "                    defaults.append(param.default)\n",
        "            elif param.kind == param.VAR_POSITIONAL:\n",
        "                varargs = name\n",
        "            elif param.kind == param.VAR_KEYWORD:\n",
        "                keywords = name\n",
        "\n",
        "        return ArgSpec(args, varargs, keywords, tuple(defaults) if defaults else None)\n",
        "\n",
        "    inspect.getargspec = getargspec\n"
      ],
      "metadata": {
        "id": "JBNPFS7tdod_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Install Repos [If done previously, then can skip this]"
      ],
      "metadata": {
        "id": "0pFGbWzAdyoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone required repositories into Brev local path\n",
        "git clone https://github.com/Vegetebird/StridedTransformer-Pose3D.git \\\n",
        "    ~/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "\n",
        "git clone https://github.com/EricGuo5513/text-to-motion.git \\\n",
        "    ~/autosynthda/indiv/text-to-motion\n",
        "\n",
        "git clone https://github.com/wangsen1312/joints2smpl.git \\\n",
        "    ~/autosynthda/indiv/joints2smpl\n",
        "\n",
        "# (Optional) Clone SlowFast if needed\n",
        "# git clone https://github.com/facebookresearch/SlowFast.git ~/autosynthda/indiv/SlowFast\n",
        "\n",
        "# Move into the working directory\n",
        "cd ~/autosynthda/indiv\n",
        "\n",
        "# Download Blender 3.0.0\n",
        "wget -P . https://download.blender.org/release/Blender3.0/blender-3.0.0-linux-x64.tar.xz\n",
        "\n",
        "# Extract Blender in the current directory\n",
        "tar -xf blender-3.0.0-linux-x64.tar.xz -C .\n"
      ],
      "metadata": {
        "id": "zYso70lldoat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change directory into the StridedTransformer repo\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/StridedTransformer-Pose3D'))\n",
        "\n",
        "# Install gdown if needed\n",
        "!pip install -q gdown\n",
        "\n",
        "# Make sure the pretrained checkpoint directory exists\n",
        "os.makedirs('checkpoint/pretrained', exist_ok=True)\n",
        "\n",
        "# Download the two refine/non-refine checkpoints if missing\n",
        "if not os.path.exists('checkpoint/pretrained/refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1aDLu0SB9JnPYZOOzQsJMV9zEIHg2Uro7 \\\n",
        "           -O checkpoint/pretrained/refine_4365.pth\n",
        "\n",
        "if not os.path.exists('checkpoint/pretrained/no_refine_4365.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1l63AI9BsNovpfTAbfAkySo9X2MOWgYZH \\\n",
        "           -O checkpoint/pretrained/no_refine_4365.pth\n",
        "\n",
        "# Ensure the demo/lib/checkpoint directory exists\n",
        "os.makedirs('demo/lib/checkpoint', exist_ok=True)\n",
        "\n",
        "# Download YOLOv3 weights\n",
        "if not os.path.exists('demo/lib/checkpoint/yolov3.weights'):\n",
        "    !gdown https://drive.google.com/uc?id=\n"
      ],
      "metadata": {
        "id": "6hKq3Q7Udg9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change directory to text-to-motion repo\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/text-to-motion'))\n",
        "\n",
        "# Ensure the checkpoints1 directory exists\n",
        "os.makedirs('checkpoints1', exist_ok=True)\n",
        "\n",
        "# Install gdown if it's not already available\n",
        "!pip install -q gdown\n",
        "\n",
        "# Download the model checkpoint if it's not already present\n",
        "if not os.path.exists('checkpoints1/checkpoints'):\n",
        "    !gdown https://drive.google.com/uc?id=12liZW5iyvoybXD8eOw4VanTgsMtynCuU \\\n",
        "           -O checkpoints1/checkpoints\n"
      ],
      "metadata": {
        "id": "EIqd-Q4ndg4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Switch into the text-to-motion folder\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/text-to-motion'))\n",
        "\n",
        "# Ensure the checkpoints1 directory exists\n",
        "os.makedirs('checkpoints1', exist_ok=True)\n",
        "\n",
        "# Install gdown if needed\n",
        "!pip install -q gdown\n",
        "\n",
        "# Download and unzip the first model if not already present\n",
        "if not os.path.exists('checkpoints1/model.zip'):\n",
        "    !gdown --id 12liZW5iyvoybXD8eOw4VanTgsMtynCuU -O checkpoints1/model.zip\n",
        "    !unzip -q checkpoints1/model.zip -d checkpoints1\n",
        "\n",
        "# Download and unzip the second model if not already present\n",
        "if not os.path.exists('checkpoints1/model2.zip'):\n",
        "    !gdown --id 1IgrFCnxeg4olBtURUHimzS03ZI0df_6W -O checkpoints1/model2.zip\n",
        "    !unzip -q checkpoints1/model2.zip -d checkpoints1\n"
      ],
      "metadata": {
        "id": "6onFvMnBd9kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Each Component for Sanity Check"
      ],
      "metadata": {
        "id": "7J5mxVLsfCFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade pip\n",
        "pip install --upgrade pip\n",
        "\n",
        "# Install PyTorch with CUDA 12.4 (adjust if needed for your GPU driver)\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install remaining dependencies\n",
        "pip install \\\n",
        "    iopath \\\n",
        "    fvcore \\\n",
        "    pytorchvideo \\\n",
        "    tensorboard \\\n",
        "    setuptools \\\n",
        "    torchinfo \\\n",
        "    opencv-python \\\n",
        "    seaborn \\\n",
        "    numpy \\\n",
        "    Pillow \\\n",
        "    scikit-learn\n"
      ],
      "metadata": {
        "id": "5dnJ-9nOXked"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the version of torch and whether cuda is used\n",
        "import torch\n",
        "print(\"Torch file:\", torch.__file__)\n",
        "print(\"CUDA attr exists:\", hasattr(torch, \"cuda\"))\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Device count:\", torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "A0plBGPtXCc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_vars = \"\"\"\n",
        "STRIDED_TRANSFORMER_PATH=~/autosynthda/indiv/StridedTransformer-Pose3D\n",
        "TEXT_TO_MOTION_PATH=~/autosynthda/indiv/text-to-motion\n",
        "JOINTS2SMPL_PATH=~/autosynthda/indiv/joints2smpl\n",
        "SLOWFAST_PATH=~/autosynthda/indiv/SlowFast\n",
        "BLENDER_BIN=~/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "BLENDER_ROOT=~/autosynthda/indiv/blender-3.0.0-linux-x64\n",
        "BLENDER_PATH=~/autosynthda/indiv/blender-3.0.0-linux-x64/blender\n",
        "\"\"\"\n",
        "\n",
        "# Expand ~ to full path\n",
        "env_path = os.path.expanduser('~/autosynthda/indiv/synthda/components/.env')\n",
        "\n",
        "# Write the .env file\n",
        "with open(env_path, \"w\") as f:\n",
        "    f.write(env_vars.strip())\n"
      ],
      "metadata": {
        "id": "0Od8Ajp5doXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Note: You should replace the files with the edited versions to make it work in Colab. It can be found in the colab/ folder on the Github\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(os.path.expanduser('~/autosynthda/indiv/synthda/components/.env'))\n",
        "\n",
        "\n",
        "# Overwrite joints2smpl/fit_seq.py with synthda's modified version\n",
        "wget -O ~/autosynthda/indiv/joints2smpl/fit_seq.py \\\n",
        "  https://raw.githubusercontent.com/NVIDIA/synthda/main/colab/synthda_mods/fit_seq.py\n",
        "\n",
        "# Overwrite text-to-motion/utils/plot_script.py with synthda's modified version\n",
        "wget -O ~/autosynthda/indiv/text-to-motion/utils/plot_script.py \\\n",
        "  https://raw.githubusercontent.com/NVIDIA/synthda/main/colab/synthda_mods/plot_script.py\n",
        "\n",
        "\n",
        "\n",
        "#Ensure sample_video.mp4 is present in the StridedTransformer-Pose3D directory â€” if it lives elsewhere, update the --video path accordingly, e.g.:\n",
        "# --video ~/autosynthda/data/my_clip.mp4\n"
      ],
      "metadata": {
        "id": "20ADxtPifB2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change to StridedTransformer-Pose3D directory\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/StridedTransformer-Pose3D'))\n",
        "\n",
        "# Run the visualization script on the sample video\n",
        "!python demo/vis.py --video sample_video.mp4\n"
      ],
      "metadata": {
        "id": "unElOjv_d9hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Switch to the text-to-motion folder\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/text-to-motion'))\n",
        "\n",
        "# Run the motion generation script\n",
        "!python gen_motion_script.py \\\n",
        "    --name Comp_v6_KLD01 \\\n",
        "    --text_file input.txt \\\n",
        "    --repeat_time 1\n"
      ],
      "metadata": {
        "id": "Y4GvZ3B3fHYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import importlib\n",
        "import smplx\n",
        "\n",
        "# Reload smplx in case of updates\n",
        "importlib.reload(smplx)\n",
        "\n",
        "# Switch to joints2smpl folder\n",
        "os.chdir(os.path.expanduser('~/autosynthda/indiv/joints2smpl'))\n",
        "\n",
        "# List the contents of the SMPL model directory\n",
        "!ls -lh ~/autosynthda/indiv/joints2smpl/smpl_models/smpl/\n",
        "\n",
        "# Run the SMPL fitting script on test_motion2.npy\n",
        "!python fit_seq.py --files test_motion2.npy\n"
      ],
      "metadata": {
        "id": "BIWjVjGEjJLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source folder: joints2smpl/demo/demo_results/test_motion2\n",
        "src = os.path.expanduser('~/autosynthda/indiv/joints2smpl/demo/demo_results/test_motion2')\n",
        "\n",
        "# Destination folder: synthda/components/renders/test_motion2\n",
        "dst = os.path.expanduser('~/autosynthda/indiv/synthda/components/renders/test_motion2')\n",
        "\n",
        "# Ensure the parent directory exists\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "\n",
        "# Remove destination if it already exists\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "# Copy the entire folder\n",
        "shutil.copytree(src, dst)\n",
        "print(\"âœ… Folder copied to expected Blender input location.\")\n",
        "\n",
        "# Verify\n",
        "print(\"Exists:\", os.path.exists(dst))\n",
        "print(\"Contents:\", os.listdir(dst) if os.path.exists(dst) else \"Path does not exist.\")\n"
      ],
      "metadata": {
        "id": "S-TBepGafHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this Brev Instance / Colab Instance, we do not use Blender as it may be too intensive. For Blender we would recc the local version of SynthDa. For strictly demo purposes we are using PyRender, but the quality will not be the same as Blender"
      ],
      "metadata": {
        "id": "aP8665QSkVZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/content/blender-3.0.0-linux-x64/blender -b -P /content/synthda/components/animation_pose.py -- --name <folder_with_ply_files>\n",
        "\n",
        "# may not work on on Colab easily as Blender is intensive + running blender headlessly on Colab has some challenges.\n",
        "# However you will be able to download the .fbx as well to view the animation locally.\n",
        "# For Colab, the suggested alternative is to use pyrender instead, which we use in the demo function created for BREV below!\n",
        "\n"
      ],
      "metadata": {
        "id": "ItzYyK-Qd9bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Your Own Demo Data [Trying out Real-Mix from SynthDa]"
      ],
      "metadata": {
        "id": "ipIi5J3oUpEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required rendering and utility libraries\n",
        "pip install \\\n",
        "    trimesh \\\n",
        "    pyrender \\\n",
        "    imageio[ffmpeg] \\\n",
        "    pyglet \\\n",
        "    av \\\n",
        "    python-dotenv\n"
      ],
      "metadata": {
        "id": "QvnYoc2xVz0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyrender\n",
        "from pyrender.platforms import egl\n",
        "print(\"EGL available:\", egl.is_available())\n"
      ],
      "metadata": {
        "id": "JJXXI2malh4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add synthda components path to sys.path (Brev local path)\n",
        "components_path = os.path.expanduser(\"~/autosynthda/indiv/synthda/components\")\n",
        "sys.path.append(components_path)\n",
        "\n",
        "from optimisation.optimisation_utils import map_h36m_to_smpl, upsample_pose_data, compute_P_opt\n",
        "\n",
        "# used when both sources are real tracked motion data\n",
        "def main_real_real(real_path_npz_1, real_path_npz_2, folder_path):\n",
        "    folder_path = Path(folder_path)\n",
        "    folder_path_variations = folder_path / \"all_variations\"\n",
        "    folder_path_variations.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Output paths\n",
        "    real_data_1_path = folder_path / 'output_keypoints_3d_real1.npy'\n",
        "    real_data_2_path = folder_path / 'output_keypoints_3d_real2.npy'\n",
        "\n",
        "    print(real_data_1_path)\n",
        "    print(real_data_2_path)\n",
        "\n",
        "    # Map .npz to SMPL-22 .npy\n",
        "    real_data_1 = map_h36m_to_smpl(real_path_npz_1)\n",
        "    np.save(real_data_1_path, real_data_1)\n",
        "\n",
        "    real_data_2 = map_h36m_to_smpl(real_path_npz_2)\n",
        "    np.save(real_data_2_path, real_data_2)\n",
        "\n",
        "    # Load data to compare frame counts\n",
        "    P_r = np.load(real_data_1_path)\n",
        "    P_s = np.load(real_data_2_path)\n",
        "\n",
        "    # Determine which array is shorter\n",
        "    if P_r.shape[0] > P_s.shape[0]:\n",
        "        smaller_array = real_data_2_path\n",
        "        bigger_array = real_data_1_path\n",
        "        max_frames = P_r.shape[0]\n",
        "    else:\n",
        "        smaller_array = real_data_1_path\n",
        "        bigger_array = real_data_2_path\n",
        "        max_frames = P_s.shape[0]\n",
        "\n",
        "    # Upsample smaller array\n",
        "    pose_data_upsampled = upsample_pose_data(str(smaller_array), target_frames=max_frames)\n",
        "    extended_new_path = smaller_array.with_stem(smaller_array.stem + \"_extended\")\n",
        "    np.save(extended_new_path, pose_data_upsampled)\n",
        "    print(\"Upsampled shape:\", pose_data_upsampled.shape)\n",
        "\n",
        "    # Generate interpolated optimised variants\n",
        "    weight_pairs = [(0.1, 0.9), (0.2, 0.8), (0.3, 0.7), (0.4, 0.6),\n",
        "                    (0.5, 0.5), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1)]\n",
        "\n",
        "    # Decide which file is the \"fixed\" longer one\n",
        "    if bigger_array == real_data_1_path:\n",
        "        fixed = real_data_1_path\n",
        "        variable = extended_new_path\n",
        "    else:\n",
        "        fixed = real_data_2_path\n",
        "        variable = extended_new_path\n",
        "\n",
        "    # Compute and save interpolations\n",
        "    for w_A, w_B in weight_pairs:\n",
        "        P_opt = compute_P_opt(str(fixed), str(variable), alpha=0.5, w_A=w_A, w_B=w_B)\n",
        "        np.save(folder_path_variations / f\"_euclidean_distances_wA{w_A}_wB{w_B}.npy\", P_opt)\n"
      ],
      "metadata": {
        "id": "Q-7XP5RnUx2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import trimesh\n",
        "import pyrender\n",
        "from PIL import Image\n",
        "\n",
        "# Try EGL platform for headless rendering on GPU-enabled Brev VM\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "\n",
        "# If EGL fails, try:\n",
        "# os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "\n",
        "# ---- Set your local paths on Brev (modify where needed) ----\n",
        "ply_dir = Path(\"~/autosynthda/indiv/synthda/components/renders/test_video_3_test_video_1_euclidean_distances_wA0.5_wB0.5\").expanduser()\n",
        "out_dir = Path(\"~/autosynthda/indiv/synthda/components/renders/testpng_video3_gen_frames\").expanduser()\n",
        "# --------------------------------------\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Build a single scene with camera and light\n",
        "scene = pyrender.Scene()\n",
        "camera = pyrender.PerspectiveCamera(yfov=math.radians(50.0))\n",
        "light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=2.0)\n",
        "scene.add(camera, pose=trimesh.transformations.translation_matrix([0, 0, 2]))\n",
        "scene.add(light,  pose=trimesh.transformations.translation_matrix([0, 0, 2]))\n",
        "\n",
        "# Initialize offscreen renderer\n",
        "r = pyrender.OffscreenRenderer(\n",
        "    viewport_width=1920,\n",
        "    viewport_height=1080,\n",
        "    point_size=1.0\n",
        ")\n",
        "\n",
        "# Load all .ply mesh files\n",
        "files = sorted(ply_dir.glob(\"*.ply\"))\n",
        "\n",
        "# Render each mesh as a frame\n",
        "for i, ply in enumerate(files):\n",
        "    mesh = trimesh.load_mesh(ply)\n",
        "    # If models need rotation (e.g., upright correction), uncomment below:\n",
        "    # mesh.apply_transform(trimesh.transformations.rotation_matrix(math.radians(90), [1, 0, 0]))\n",
        "\n",
        "    m = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
        "\n",
        "    # Remove previous mesh nodes\n",
        "    for node in list(scene.mesh_nodes):\n",
        "        scene.remove_node(node)\n",
        "    scene.add(m)\n",
        "\n",
        "    # Render and save frame\n",
        "    color, _ = r.render(scene)\n",
        "    Image.fromarray(color).save(out_dir / f\"frame_{i:04d}.png\")\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Rendered {i+1}/{len(files)} frames\")\n",
        "\n",
        "# Cleanup renderer\n",
        "r.delete()\n",
        "print(\"âœ… All frames saved in\", out_dir)\n"
      ],
      "metadata": {
        "id": "hiNriM4CU8s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "import math\n",
        "import trimesh\n",
        "import pyrender\n",
        "from PIL import Image\n",
        "\n",
        "# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "env_path = Path(\"~/autosynthda/indiv/synthda/components/.env\").expanduser()\n",
        "env = dotenv_values(env_path)\n",
        "\n",
        "STRIDED     = Path(env[\"STRIDED_TRANSFORMER_PATH\"]).expanduser()\n",
        "TEXT2M      = Path(env[\"TEXT_TO_MOTION_PATH\"]).expanduser()\n",
        "JOINTS2SMPL = Path(env[\"JOINTS2SMPL_PATH\"]).expanduser()\n",
        "COMP_ROOT   = Path(\"~/autosynthda/indiv/synthda/components\").expanduser()\n",
        "ANIM_REND   = COMP_ROOT / \"renders\"\n",
        "\n",
        "from optimisation.optimisation_utils import map_h36m_to_smpl, upsample_pose_data, compute_P_opt\n",
        "\n",
        "# â”€â”€â”€ Helper: headless PLYâ†’PNG via pyrender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def render_ply_sequence(ply_folder: Path, png_out: Path,\n",
        "                        width=1920, height=1080, fps=27):\n",
        "    os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "    png_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    scene = pyrender.Scene()\n",
        "    cam = pyrender.PerspectiveCamera(yfov=math.radians(50.0))\n",
        "    light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=2.0)\n",
        "    scene.add(cam, pose=trimesh.transformations.translation_matrix([0, 0, 2]))\n",
        "    scene.add(light, pose=trimesh.transformations.translation_matrix([0, 0, 2]))\n",
        "\n",
        "    r = pyrender.OffscreenRenderer(viewport_width=width,\n",
        "                                   viewport_height=height,\n",
        "                                   point_size=1.0)\n",
        "\n",
        "    ply_files = sorted(ply_folder.glob(\"*.ply\"))\n",
        "    for idx, ply in enumerate(ply_files):\n",
        "        mesh = trimesh.load_mesh(str(ply))\n",
        "        # Uncomment if upright correction is needed:\n",
        "        # mesh.apply_transform(trimesh.transformations.rotation_matrix(math.radians(90), [1, 0, 0]))\n",
        "\n",
        "        m = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
        "        for node in list(scene.mesh_nodes):\n",
        "            scene.remove_node(node)\n",
        "        scene.add(m)\n",
        "\n",
        "        color, _ = r.render(scene)\n",
        "        Image.fromarray(color).save(png_out / f\"frame_{idx:04d}.png\")\n",
        "\n",
        "    r.delete()\n",
        "\n",
        "# â”€â”€â”€ Main pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def both_real_main(*, weight_A, input_dir, output_dir, num_pairs):\n",
        "    random.seed(42)\n",
        "    video_dir = Path(input_dir).expanduser()\n",
        "    out_root = Path(output_dir).expanduser()\n",
        "    video_gen_root = video_dir.parent / f\"videos_generated_real2_{weight_A}\"\n",
        "    video_gen_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    vids = list(video_dir.glob(\"*.mp4\"))\n",
        "    pairs = list(combinations(vids, 2))\n",
        "    selection = random.sample(pairs, min(num_pairs, len(pairs)))\n",
        "\n",
        "    for v1_path, v2_path in selection:\n",
        "        v1, v2 = v1_path.name, v2_path.name\n",
        "        pair_name = v1_path.stem + \"_\" + v2_path.stem\n",
        "        pair_folder = out_root / pair_name\n",
        "        pair_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        shutil.copy(v1_path, pair_folder / v1)\n",
        "        shutil.copy(v2_path, pair_folder / v2)\n",
        "\n",
        "        # 1. StridedTransformer tracking\n",
        "        for vn in (v1, v2):\n",
        "            tgt = STRIDED / \"demo\" / \"video\" / vn\n",
        "            tgt.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(pair_folder / vn, tgt)\n",
        "            subprocess.run([\"python\", \"demo/vis.py\", \"--video\", vn], cwd=STRIDED, check=True)\n",
        "\n",
        "        # 2. Convert NPZ â†’ SMPL .npy\n",
        "        npz1 = STRIDED / \"demo\" / \"output\" / v1_path.stem / \"output_3D\" / \"output_keypoints_3d.npz\"\n",
        "        npz2 = STRIDED / \"demo\" / \"output\" / v2_path.stem / \"output_3D\" / \"output_keypoints_3d.npz\"\n",
        "        out1 = pair_folder / \"output_keypoints_3d_real1.npy\"\n",
        "        out2 = pair_folder / \"output_keypoints_3d_real2.npy\"\n",
        "\n",
        "        p1 = map_h36m_to_smpl(str(npz1)); np.save(out1, p1)\n",
        "        p2 = map_h36m_to_smpl(str(npz2)); np.save(out2, p2)\n",
        "\n",
        "        if p1.shape[0] < p2.shape[0]:\n",
        "            p1 = upsample_pose_data(str(out1), p2.shape[0]); np.save(out1, p1)\n",
        "        else:\n",
        "            p2 = upsample_pose_data(str(out2), p1.shape[0]); np.save(out2, p2)\n",
        "\n",
        "        # 3. Generate interpolated mixes\n",
        "        var_folder = pair_folder / \"all_variations\"\n",
        "        var_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for wA in [round(x, 1) for x in np.linspace(0.1, 0.9, 9)]:\n",
        "            wB = round(1 - wA, 1)\n",
        "            Popt = compute_P_opt(str(out1), str(out2), alpha=0.5, w_A=wA, w_B=wB)\n",
        "            np.save(var_folder / f\"euclidean_distances_wA{wA}_wB{wB}.npy\", Popt)\n",
        "\n",
        "        # 4. Pick selected mix and run joints2smpl\n",
        "        target_npy = var_folder / f\"euclidean_distances_wA{weight_A}_wB{round(1 - weight_A, 1)}.npy\"\n",
        "        if not target_npy.exists():\n",
        "            raise FileNotFoundError(f\"Missing variation: {target_npy}\")\n",
        "\n",
        "        dest_npy = JOINTS2SMPL / \"demo\" / \"demo_data\" / (pair_name + \"_\" + target_npy.name)\n",
        "        dest_npy.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy(target_npy, dest_npy)\n",
        "\n",
        "        subprocess.run([\n",
        "            \"python\", \"fit_seq.py\",\n",
        "            \"--files\", dest_npy.name,\n",
        "            \"--num_smplify_iters\", \"1\"\n",
        "        ], cwd=JOINTS2SMPL, check=True)\n",
        "\n",
        "        ply_src = JOINTS2SMPL / \"demo\" / \"demo_results\" / dest_npy.stem\n",
        "        if not ply_src.exists():\n",
        "            raise FileNotFoundError(f\"joints2smpl output not found: {ply_src}\")\n",
        "\n",
        "        # 5. Render PLY â†’ PNG\n",
        "        png_folder = ANIM_REND / f\"{pair_name}_{target_npy.stem}\"\n",
        "        render_ply_sequence(ply_src, png_folder)\n",
        "\n",
        "        # 6. Assemble PNGs into MP4 using ffmpeg\n",
        "        out_mp4 = video_gen_root / f\"{pair_name}.mp4\"\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-y\",\n",
        "            \"-framerate\", str(27),\n",
        "            \"-i\", str(png_folder / \"frame_%04d.png\"),\n",
        "            \"-c:v\", \"libx264\",\n",
        "            \"-pix_fmt\", \"yuv420p\",\n",
        "            str(out_mp4)\n",
        "        ], check=True)\n",
        "\n",
        "        print(\"âœ… Generated:\", out_mp4)\n"
      ],
      "metadata": {
        "id": "EpuhtZenU-93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_real_main(\n",
        "    weight_A=0.2,\n",
        "    input_dir=\"~/autosynthda/indiv/synthda/components/dataset/specific_data\",\n",
        "    output_dir=\"~/autosynthda/indiv/synthda/components/dataset/data_manipulation\",\n",
        "    num_pairs=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "tFEmo9DJVAlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u13y73EYWC2A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}